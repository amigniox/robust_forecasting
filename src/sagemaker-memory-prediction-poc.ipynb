{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training memory forecasting model on bitbrains traces\n",
    "This notebooks trains a memory forecasting model with DeepAR on AWS SageMaker\n",
    "It follows Jessie's example here: https://github.com/manifoldco/Time-Series-Forecasting-/blob/master/Manifold_AWS_DeepAR.py.ipynb\n",
    "\n",
    "The data comes from the grid-workloads-archive and is the bitbrains business-critical workloads traces: gwa-t-12\n",
    "\n",
    "This notebook will train download the data needed, munge it to timeseries, train a model, and test it. Just run all the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'manifoldco-sagemaker'\n",
    "prefix = 'hlnr-o/memory-forecasting/poc/'\n",
    "S3_MODEL_PATH = os.path.join('s3://', bucket, prefix)\n",
    "S3_DATA_PATH = os.path.join(S3_MODEL_PATH, 'data')\n",
    "\n",
    "DEEPAR_IMAGE = '522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://gwa.ewi.tudelft.nl/fileadmin/pds/trace-archives/grid-workloads-archive/datasets/gwa-t-12/rnd.zip\n",
    "!unzip rnd.zip -d ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Transform it to Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_files = glob.glob(os.path.join('./data/rnd/2013-*', \"*.csv\"))\n",
    "traces = pd.concat([pd.read_csv(fp, sep=';\\t').assign(VM=os.path.basename(fp).split('.')[0]) for fp in trace_files], ignore_index=True)\n",
    "traces.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = '1min'\n",
    "context_length = 30\n",
    "prediction_length = 30\n",
    "\n",
    "# generate timesampes\n",
    "traces['Timestamp'] = pd.to_datetime(traces['Timestamp [ms]'], unit = 's')\n",
    "traces.set_index('Timestamp', inplace=True)\n",
    "\n",
    "# generate index and targets\n",
    "traces[\"start\"] = traces.index\n",
    "traces['target'] = traces['Memory usage [KB]']\n",
    "\n",
    "# sample for every minute for every vm\n",
    "trace_timeseries  =  traces.groupby('VM').resample(freq, how={'target':np.mean})\n",
    "trace_timeseries.reset_index(level=0, inplace=True)\n",
    "\n",
    "# propogate non-null values forward and backward\n",
    "trace_timeseries  = trace_timeseries.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build train / test data sets by VM\n",
    "timeseries_test, timeseries_training = [], []\n",
    "vm_index_range = trace_timeseries['VM'].unique()\n",
    "for i in vm_index_range:\n",
    "    \n",
    "    newseries = trace_timeseries[trace_timeseries['VM'] == i]['target']\n",
    "    del newseries.index.name\n",
    "    \n",
    "    newseries.index = pd.to_datetime(newseries.index)\n",
    "    timeseries_test.append(newseries)\n",
    "    timeseries_training.append(newseries[:-prediction_length])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3filesystem = s3fs.S3FileSystem()\n",
    "encoding = 'utf-8'\n",
    "\n",
    "def series_to_jsonline(ts):\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": list(ts)}\n",
    "    return json.dumps(obj)\n",
    "\n",
    "with s3filesystem.open(os.path.join(S3_DATA_PATH, \"train\", \"test_data.json\"), 'wb') as fp:\n",
    "    for ts in timeseries_test:\n",
    "        fp.write(series_to_jsonline(ts).encode(encoding))\n",
    "        fp.write('\\n'.encode(encoding))\n",
    "        \n",
    "\n",
    "with s3filesystem.open(os.path.join(S3_DATA_PATH, \"test\", \"train_data.json\"), 'wb') as fp:\n",
    "    for ts in timeseries_training:\n",
    "        fp.write(series_to_jsonline(ts).encode(encoding))\n",
    "        fp.write('\\n'.encode(encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role='arn:aws:iam::223261615538:role/terraform-sagemaker-role',\n",
    "    image_name=DEEPAR_IMAGE,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.4xlarge',\n",
    "    base_job_name='memory-forecasting-poc',\n",
    "    output_path=S3_MODEL_PATH\n",
    ")\n",
    "\n",
    "hyperparameters  = {\n",
    "    \"time_freq\": freq,\n",
    "    \"context_length\": context_length,\n",
    "    \"prediction_length\": prediction_length,\n",
    "    \"num_cells\": \"32\",\n",
    "    \"num_layers\": \"2\",\n",
    "    \"likelihood\": \"student-t\",\n",
    "    \"epochs\": \"20\",\n",
    "    \"mini_batch_size\": \"32\",\n",
    "    \"learning_rate\": \"0.001\",\n",
    "    \"dropout_rate\": \"0.05\",\n",
    "    \"early_stopping_patience\": \"10\"\n",
    "}\n",
    "\n",
    "estimator.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "data_channels = {\n",
    "    \"train\": \"{}/train/\".format(S3_DATA_PATH),\n",
    "    \"test\": \"{}/test/\".format(S3_DATA_PATH)\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an endpoint and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = estimator.latest_training_job.name\n",
    "\n",
    "endpoint_name = sagemaker_session.endpoint_from_job(\n",
    "    job_name=job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    deployment_image=DEEPAR_IMAGE,\n",
    "    role='arn:aws:iam::223261615538:role/terraform-sagemaker-role'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "\n",
    "    def set_prediction_parameters(self, freq, prediction_length):\n",
    "        \"\"\"Set the time frequency and prediction length parameters. This method **must** be called\n",
    "        before being able to use `predict`.\n",
    "        \n",
    "        Parameters:\n",
    "        freq -- string indicating the time frequency\n",
    "        prediction_length -- integer, number of predicted time points\n",
    "        \n",
    "        Return value: none.\n",
    "        \"\"\"\n",
    "        self.freq = freq\n",
    "        self.prediction_length = prediction_length\n",
    "        \n",
    "    def predict(self, ts, cat=None, encoding=\"utf-8\", num_samples=100, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        Parameters:\n",
    "        ts -- list of `pandas.Series` objects, the time series to predict\n",
    "        cat -- list of integers (default: None)\n",
    "        encoding -- string, encoding to use for the request (default: \"utf-8\")\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_times = [x.index[-1]+1 for x in ts]\n",
    "        req = self.__encode_request(ts, cat, encoding, num_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, prediction_times, encoding)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, encoding, num_samples, quantiles):\n",
    "        instances = [series_to_obj(ts[k], cat[k] if cat else None) for k in range(len(ts))]\n",
    "        configuration = {\"num_samples\": num_samples, \"output_types\": [\"quantiles\"], \"quantiles\": quantiles}\n",
    "        http_request_data = {\"instances\": instances, \"configuration\": configuration}\n",
    "        return json.dumps(http_request_data).encode(encoding)\n",
    "    \n",
    "    def __decode_response(self, response, prediction_times, encoding):\n",
    "        response_data = json.loads(response.decode(encoding))\n",
    "        list_of_df = []\n",
    "        for k in range(len(prediction_times)):\n",
    "            prediction_index = pd.DatetimeIndex(start=prediction_times[k], freq=self.freq, periods=self.prediction_length)\n",
    "            list_of_df.append(pd.DataFrame(data=response_data['predictions'][k]['quantiles'], index=prediction_index))\n",
    "        return list_of_df\n",
    "\n",
    "    \n",
    "def series_to_obj(ts, cat=None):\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": list(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor  = DeepARPredictor(\n",
    "    endpoint=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    content_type=\"application/json\"\n",
    ")\n",
    "predictor.set_prediction_parameters(freq, prediction_length)\n",
    "\n",
    "new_time_series_training = []\n",
    "for ts in timeseries_training:\n",
    "    new_time_series_training.append(ts.asfreq('T'))\n",
    "\n",
    "new_time_series_test = []\n",
    "for ts in timeseries_test:\n",
    "    new_time_series_test.append(ts.asfreq('T'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_df  = predictor.predict(new_time_series_training[100:101]) # predicted forecast\n",
    "actual_data = new_time_series_test[100:101] # full data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for k in range(len(list_of_df)): \n",
    "    plt.figure(figsize=(12,6))\n",
    "    actual_data[k][-prediction_length-context_length:].plot(label='Actual',linewidth = 2.5)\n",
    "    p10 = list_of_df[k]['0.1'] \n",
    "    p90 = list_of_df[k]['0.9'] #set limits predictively\n",
    "    plt.fill_between(p10.index, p10, p90, alpha=0.5, label='80% Confidence Interval')\n",
    "    list_of_df[k]['0.5'].plot(label='Prediction Median', color = 'orange',linewidth = 2.5) # set requests for capacity allocation \n",
    "    plt.title(\"DeepAR Model Prediction\", fontsize = 23)\n",
    "    plt.ylabel(\"Memory Usage [KBs]\", fontsize = 20)\n",
    "    #plt.yticks([10,20.40,50])\n",
    "    plt.xlabel(\"Time\", fontsize = 20)\n",
    "    (list_of_df[k]['0.9']+100).plot(label='My Suggested Provision', color = 'g',linewidth = 2.5) # set requests for capacity allocation \n",
    "    plt.yticks(fontsize=14);\n",
    "    #plt.axhline(y=5851.99912, color='r', linestyle='-', label = 'Actual Provision')\n",
    "    plt.xticks(fontsize=14);\n",
    "    plt.legend(fontsize = 12,loc = 'best')\n",
    "    #plt.savefig('VM101-withactual')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.delete_endpoint(endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
