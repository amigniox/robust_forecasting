{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series forecasting with DeepAR - Wikimedia pageview data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the set of parameters with smallest RMSE during the the hyperparameter tuning job. This notebook trains the model with this set of hyperparameters. The SMAPE scores improved around 20% for all time series buckets.\n",
    "\n",
    "- This notebook we train a deepAR model using 2Years/ over all available of wiki-projects pageview data within requested time range, and test the model with both wiki and synthetic data.  \n",
    "- Model pipeline: Request to Wiki REST api >> save_jason() to S3 bucket >> train model >> deploy trained model on Sagemaker, create endpoint >> request inference >> get prediction >> evaluation with test metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "#from preprocessor import save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "bucket = 'sagemaker-deepar20190120'\n",
    "prefix = 'sagemaker/wiki-test-deepar'\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "s3_data_path = \"{}/{}/data\".format(bucket, prefix)\n",
    "s3_output_path = \"{}/{}/output\".format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "image_name = get_image_uri(boto3.Session().region_name, 'forecasting-deepar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict the next 48 points of time series.\n",
    "The time series that we use have hourly granularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 'H'\n",
    "prediction_length = 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`context_length`: how many previous points to look at. A typical value to start with is around the same size as the `prediction_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 118"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful functions convert `pandas.Series` objects into the appropriate JSON strings that DeepAR can consume. We will use these to write the data to S3. (didn't call during this example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_obj(ts, cat=None):\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": list(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    return obj\n",
    "\n",
    "def series_to_jsonline(ts, cat=None):\n",
    "    return json.dumps(series_to_obj(ts, cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get wikimedia pageview data from its REST API\n",
    "We request the http requests sent by users to wiki projects. The output will be a json file with required format, each line contains hourly pageview numbers from `start` to `end`. Output files are saved in a S3 bucket.\n",
    "- (A side note: wiki api allows <b>100</b> requests/s maximum, don't get blacklisted.)\n",
    "- (Anote side note: the response will be json objects, each has a time stamp and corresponding time series values such as pageview, the maximum number of returned objects is 5000 that's why I use time delta = 200 days = 4800 objects for each request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "all_data = []\n",
    "project_list = []\n",
    "delta = datetime.timedelta(days=199)\n",
    "hour = datetime.timedelta(hours=1)\n",
    "zero = datetime.timedelta(hours=0)\n",
    "\n",
    "\n",
    "def save_json(start, end, out_path, input_path):\n",
    "    s3filesystem = s3fs.S3FileSystem()\n",
    "    start_date = datetime.datetime.strptime(start, '%Y%m%d')\n",
    "    end_date = datetime.datetime.strptime(end, '%Y%m%d')\n",
    "    if (end_date - start_date).days < 0:\n",
    "        raise Exception('start date should NOT after end date')\n",
    "\n",
    "    expected_size = ((end_date - start_date).days + 1) * 24\n",
    "\n",
    "    # provide wiki-project list here!\n",
    "    with open(input_path) as f:\n",
    "        projects = f.read().splitlines()\n",
    "\n",
    "    for item in projects:\n",
    "        data = get_single_views(item, start_date, end_date)\n",
    "        if len(data['target']) == expected_size:\n",
    "            all_data.append(data)\n",
    "            project_list.append(item)\n",
    "\n",
    "    # convert all items to JSON and write to a file\n",
    "    print('saving data')\n",
    "    print(project_list)\n",
    "    with open(input_path[:-4] + '-' + str(start) + '-' + str(end) + '-get.txt', 'w') as f:\n",
    "        for item in project_list:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    with s3filesystem.open(out_path, 'w') as outfile:\n",
    "        outfile.write('\\n'.join(json.dumps(i) for i in all_data) + '\\n')\n",
    "    all_data.clear()\n",
    "    project_list.clear()\n",
    "\n",
    "\n",
    "def get_single_views(item, start_date, end_date):\n",
    "    base_url = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/aggregate/' \\\n",
    "               + item \\\n",
    "               + '/all-access/user/hourly/'\n",
    "    # dict is used to save JSON of each domain\n",
    "    data = dict()\n",
    "    data['start'] = start_date.strftime('%Y-%m-%d') + ' 00:00:00'\n",
    "    data['target'] = []\n",
    "\n",
    "    while start_date <= end_date:\n",
    "        if start_date + delta <= end_date:\n",
    "            final_url = base_url + start_date.strftime('%Y%m%d') + '00/' \\\n",
    "                        + (start_date + delta).strftime('%Y%m%d') + '23'\n",
    "            end_check = start_date + delta + 23 * hour\n",
    "        else:\n",
    "            final_url = base_url + start_date.strftime('%Y%m%d') + '00/' \\\n",
    "                        + end_date.strftime('%Y%m%d') + '23'\n",
    "            end_check = end_date + 23 * hour\n",
    "        # make a API request here!!!\n",
    "        response = requests.get(final_url)\n",
    "        time.sleep(0.05)\n",
    "        try:\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(\"<-----Error---->: \" + final_url + str(e))\n",
    "            break\n",
    "        print('<-----OK----->' + final_url)  # debug\n",
    "        output = response.json()\n",
    "        print('processed data points: ' + str(len(output['items'])))  # debug\n",
    "        temp = start_date - hour\n",
    "        for i in output['items']:\n",
    "            curr = datetime.datetime.strptime(i['timestamp'], '%Y%m%d%H')\n",
    "            while curr - temp > hour:\n",
    "                data['target'].append(None)\n",
    "                temp += hour\n",
    "            data['target'].append(i['views'])\n",
    "            temp = curr\n",
    "\n",
    "        while (end_check - temp) > zero:\n",
    "            data['target'].append(None)\n",
    "            temp += hour\n",
    "\n",
    "        start_date += datetime.timedelta(days=200)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = '20160101'\n",
    "train_end = '20171231'\n",
    "save_json(train_start,train_end,s3_data_path + \"/train_0toNaN/train.json\", 'wp_full.txt')\n",
    "test_start = '20180101'\n",
    "test_end = '20181231'\n",
    "save_json(test_start,test_end,s3_data_path + \"/test_0toNaN/test.json\", 'wp_full.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.xlarge',\n",
    "    base_job_name='DEMO-deepar',\n",
    "    output_path=\"s3://\" + s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"num_cells\": \"35\",\n",
    "    \"num_layers\": \"2\",\n",
    "    \"likelihood\": \"student-t\",\n",
    "    \"epochs\": \"39\",\n",
    "    \"mini_batch_size\": \"85\",\n",
    "    \"learning_rate\": \"0.0030902721170490166\",\n",
    "    \"dropout_rate\": \"0.052384954005170334\",\n",
    "    \"early_stopping_patience\": \"10\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_channels = {\n",
    "    \"train\": \"s3://{}/train_0toNaN/\".format(s3_data_path),\n",
    "    \"test\": \"s3://{}/test_0toNaN/\".format(s3_data_path)\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint and predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = estimator.latest_training_job.name\n",
    "\n",
    "endpoint_name = sagemaker_session.endpoint_from_job(\n",
    "    job_name=job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    deployment_image=image_name,\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "\n",
    "    def set_prediction_parameters(self, freq, prediction_length):\n",
    "        \"\"\"Set the time frequency and prediction length parameters. This method **must** be called\n",
    "        before being able to use `predict`.\n",
    "        \n",
    "        Parameters:\n",
    "        freq -- string indicating the time frequency\n",
    "        prediction_length -- integer, number of predicted time points\n",
    "        \n",
    "        Return value: none.\n",
    "        \"\"\"\n",
    "        self.freq = freq\n",
    "        self.prediction_length = prediction_length\n",
    "        \n",
    "    def predict(self, ts, cat=None, encoding=\"utf-8\", num_samples=100, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        Parameters:\n",
    "        ts -- list of `pandas.Series` objects, the time series to predict\n",
    "        cat -- list of integers (default: None)\n",
    "        encoding -- string, encoding to use for the request (default: \"utf-8\")\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_times = [x.index[-1]+1 for x in ts]\n",
    "        req = self.__encode_request(ts, cat, encoding, num_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, prediction_times, encoding)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, encoding, num_samples, quantiles):\n",
    "        instances = [series_to_obj(ts[k], cat[k] if cat else None) for k in range(len(ts))]\n",
    "        configuration = {\"num_samples\": num_samples, \"output_types\": [\"quantiles\"], \"quantiles\": quantiles}\n",
    "        http_request_data = {\"instances\": instances, \"configuration\": configuration}\n",
    "        return json.dumps(http_request_data).encode(encoding)\n",
    "    \n",
    "    def __decode_response(self, response, prediction_times, encoding):\n",
    "        response_data = json.loads(response.decode(encoding))\n",
    "        list_of_df = []\n",
    "        for k in range(len(prediction_times)):\n",
    "            prediction_index = pd.DatetimeIndex(start=prediction_times[k], freq=self.freq, periods=self.prediction_length)\n",
    "            list_of_df.append(pd.DataFrame(data=response_data['predictions'][k]['quantiles'], index=prediction_index))\n",
    "        return list_of_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = DeepARPredictor(\n",
    "    endpoint=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    content_type=\"application/json\"\n",
    ")\n",
    "predictor.set_prediction_parameters(freq, prediction_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions, plot and evaluate the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make prediction for `wiki` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = \"s3://{}/test/test.json\".format(s3_data_path)\n",
    "df_ts = pd.read_json(data_location, lines = True)\n",
    "df_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pt = len(df_ts.iloc[1,1])\n",
    "num_ts = len(df_ts)\n",
    "\n",
    "time_series_wiki = [] \n",
    "for k in range(num_ts-1):\n",
    "    t0 = df_ts.iloc[k,0]\n",
    "    data = df_ts.iloc[k,1]\n",
    "    index = pd.DatetimeIndex(start=t0, freq=freq, periods=num_pt)\n",
    "    time_series_wiki.append(pd.Series(data=data, index=index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_wiki_predict = []\n",
    "for ts in time_series_wiki:\n",
    "    time_series_wiki_predict.append(ts[:-prediction_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result of prediction will be stored in list_of_wiki_pred\n",
    "list_of_wiki_pred = []\n",
    "for i in range(0,len(time_series_wiki_predict)):\n",
    "    list_of_wiki_pred.append(predictor.predict(time_series_wiki_predict[i:i+1]))\n",
    "    # The following line is added to avoid time out error.\n",
    "    time.sleep(0.01) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick overview of the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample several predicted time series use range(0, len(list_of_wiki_pred), 50)\n",
    "for k in range(0, len(list_of_wiki_pred), 50):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    time_series_wiki[k][-prediction_length-context_length:].plot(label='target')\n",
    "    p10 = list_of_wiki_pred[k][0]['0.1']\n",
    "    p90 = list_of_wiki_pred[k][0]['0.9']\n",
    "    plt.fill_between(p10.index, p10, p90, color='y', alpha=0.5, label='80% confidence interval')\n",
    "    list_of_wiki_pred[k][0]['0.5'].plot(label='prediction median')\n",
    "    plt.legend(loc = 'best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics to evaluate the prediction\n",
    "- Mean absolute percentage error\n",
    "- Mean absolute error with log1p\n",
    "- Symmetric mean absolute percentage error\n",
    "- Symmetric mean absolute percentage error differentiable\n",
    "- others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximated differentiable SMAPE\n",
    "def differentiable_smape(true, predicted):\n",
    "    epsilon = 0.1\n",
    "    true_o = true\n",
    "    pred_o = predicted\n",
    "    summ = np.maximum(np.abs(true_o) + np.abs(pred_o) + epsilon, 0.5 + epsilon)\n",
    "    smape = np.abs(pred_o - true_o) / summ\n",
    "    return smape\n",
    "\n",
    "# SMAPE, rounded up to the closest integer\n",
    "def rounded_smape(true, predicted):\n",
    "    true_o = np.int(np.round(true))\n",
    "    pred_o = np.round(predicted).astype(np.int32)\n",
    "    summ = np.abs(true_o) + np.abs(pred_o)\n",
    "    smape = np.where(summ==0, 0, np.abs(pred_o - true_o) / summ)\n",
    "    return smape\n",
    "\n",
    "# SMAPE stardard definition\n",
    "def smape(true, predicted):\n",
    "    true_o = true\n",
    "    pred_o = predicted\n",
    "    summ = np.abs(true_o) + np.abs(pred_o)\n",
    "    smape = np.where(summ==0, 0, np.abs(pred_o - true_o) / summ)\n",
    "    return smape\n",
    "\n",
    "# MAE standard definition\n",
    "def mape(true, predicted):\n",
    "    true_o = true\n",
    "    pred_o = predicted\n",
    "    denom = np.abs(true_o)\n",
    "    mape = np.where(denom==0, 0, np.abs(pred_o - true_o) / denom)\n",
    "    return mape\n",
    "\n",
    "\n",
    "# MAE on log1p\n",
    "def mape1p(true, predicted):\n",
    "    epsilon = 0.1\n",
    "    true_o = np.log1p(true + epsilon)\n",
    "    pred_o = np.log1p(predicted + epsilon)\n",
    "    error = np.where(np.abs(true_o)==0, 0, np.abs(pred_o - true_o) / np.abs(true_o))\n",
    "#     error = np.abs(true_o - pred_o)/np.abs(true_o)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input pd.series in which index = time stamps, value = time series values\n",
    "# actual_series and pred_series should have exactly matched timestamps\n",
    "\n",
    "def plot_metrics(actual_series, pred_series):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    metric_smape = differentiable_smape(actual_series.values, pred_series.values)\n",
    "    metric_mape = mape(actual_series.values, pred_series.values)\n",
    "    metric_maelog1p = mape1p(actual_series.values, pred_series.values)\n",
    "    metric_smape_orig = smape(actual_series.values, pred_series.values)\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(pred_series.index, metric_smape_orig, label = 'smape standard = {}'.format(metric_smape_orig.mean()))  \n",
    "    plt.plot(pred_series.index, metric_smape, label = 'smape differentialble = {}'.format(metric_smape.mean()))\n",
    "    plt.plot(pred_series.index, metric_mape, label = 'mape standard = {}'.format(metric_mape.mean()))\n",
    "    # plt.plot(pred_series.index, metric_maelog1p, label = 'mape with log(1 + epsilon +val) = {}'.format(metric_maelog1p.mean()))\n",
    "    plt.xlabel('Time stamp of prediction')\n",
    "    plt.ylabel('Prediction metric')\n",
    "    plt.legend(loc = 'best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the metrics for wiki data prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(0, len(list_of_wiki_pred), 50):\n",
    "    plot_metrics(time_series_wiki[k][-prediction_length:], list_of_wiki_pred[k][0]['0.5'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "# This function characterize time series with FFT.\n",
    "def characterize_ts(ts,window):\n",
    "    y = ts.values # signal\n",
    "    Fs = 1 # sampling rate, in our case let's use 1 Hour^-1\n",
    "    n = len(y) # length of the signal\n",
    "    k = np.arange(n)\n",
    "    T = n/Fs\n",
    "    frq = k/T # two sides frequency range\n",
    "    # We only keep the positive frequency up to the Nyquist = 1/(2*dT), dT = sampling interval.\n",
    "    cycle = frq[range(1,int(n/2))] * window # one side frequency range\n",
    "\n",
    "    Y = np.fft.fft(y)/n # fft computing and normalization\n",
    "    Y = Y[range(1, int(n/2))] # again, spectrum corrresponding to the positive half\n",
    "    yabs = np.abs(Y)\n",
    "    \n",
    "    # Locate the largest 15 peaks, use them to characterise the time series.\n",
    "    indx = heapq.nlargest(15, range(len(yabs)), yabs.__getitem__)\n",
    "    amp = heapq.nlargest(15,yabs)\n",
    "\n",
    "    mean = yabs.mean()\n",
    "    std = yabs.std()\n",
    "    cyc_hday = 2.0\n",
    "    cyc_day = cyc_hday / 2.0\n",
    "    cyc_week = cyc_day / 7.0\n",
    "    cyc_month = cyc_day / 30.0\n",
    "\n",
    "    comp = lambda a,b : np.abs(a/b - 1) < 0.05\n",
    "    ts_type = ['trend', 'hDay', 'Day', 'Week', 'Month', 'DayImpulse', 'spike']\n",
    "    report_list = [0] * 8\n",
    "    for counter, value in enumerate(indx):\n",
    "        # Here we define a peak in frequency domain.\n",
    "        if amp[counter] > (mean + 3*std):\n",
    "            amp_norm = (amp[counter] - mean)/std\n",
    "            if cycle[value] < 0.01:\n",
    "                # trend (increasing, decreasing, gaussian pulse)\n",
    "                report_list[0] = max(amp_norm,report_list[0])\n",
    "            elif comp(cycle[value], cyc_hday):\n",
    "                report_list[1] = max(amp_norm,report_list[1])\n",
    "            elif comp(cycle[value], cyc_day):\n",
    "                report_list[2] = max(amp_norm,report_list[2])\n",
    "            elif comp(cycle[value], cyc_week):\n",
    "                report_list[3] = max(amp_norm,report_list[3])\n",
    "            elif comp(cycle[value], cyc_month):\n",
    "                report_list[4] = max(amp_norm,report_list[4])\n",
    "\n",
    "    if sum(report_list[:5]) > 0:\n",
    "        index = report_list[:5].index(max(report_list[:5])) \n",
    "        report_list[5] = ts_type[index]\n",
    "    else: \n",
    "        report_list[5] = ts_type[-1]\n",
    "        \n",
    "    # Add a subcategory: a special day seasonality that has a periodic impulse shape.    \n",
    "    if (report_list[5] == 'hDay' or report_list[5] == 'Day'):\n",
    "        harmonic = set(range(12))\n",
    "        all_peak = set()\n",
    "        for counter, value in enumerate(indx):\n",
    "            if amp[counter] > (mean + 3*std):\n",
    "                all_peak.add(int(round(cycle[value])))\n",
    "        if len(harmonic.intersection(all_peak)) > 10 :\n",
    "            report_list[5] = ts_type[-2]\n",
    "\n",
    "    report_list[-2] = y.mean()\n",
    "    report_list[-1] = y.std()\n",
    "    return report_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_list = []\n",
    "day = 24\n",
    "for item in time_series_wiki:\n",
    "    character_list.append(characterize_ts(item,day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_list = []\n",
    "for i in range(len(list_of_wiki_pred)):\n",
    "    metric_smape = differentiable_smape(time_series_wiki[i][-prediction_length:].values, \n",
    "                                        list_of_wiki_pred[i][0]['0.5'].values)\n",
    "    error_list.append(metric_smape.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By classifying the time series into different buckets, we can see how the model performs on different types of time series.\n",
    "\n",
    "- The major types of time series in the wiki dataset are: Day seasonality, Trend seasonality. We also observe a small population of HalfDay seasonality time series as well.\n",
    "\n",
    "- The SMAPE score for the two major types Day seasonality = 20.95% and Trend = 32.52% is still decent (in comparison with the Kaggle competition for wiki day page view best SMAPE = 35.48065%).\n",
    "\n",
    "- But we see the model performance varies to some extent according to the scale or type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = {}\n",
    "# we have ['trend', 'hDay', 'Day', 'Week', 'Month', 'DayImpulse', 'spike']\n",
    "ans['trend'] = []\n",
    "ans['hDay'] = []\n",
    "ans['Day'] = []\n",
    "ans['Week'] = []\n",
    "ans['Month'] = []\n",
    "ans['spike'] = []\n",
    "ans['DayImpulse'] = []\n",
    "\n",
    "for i in range(len(error_list)):\n",
    "    ans[character_list[i][5]].append(error_list[i])\n",
    "\n",
    "plt.figure(figsize=(12,8))  \n",
    "for key, value in ans.items():\n",
    "    if value:\n",
    "        print(key, ' smape ', np.mean(value), ' number of series ', len(value))\n",
    "        plt.bar(key, np.mean(value), label = 'number of {} = {}'.format(key, len(value)))\n",
    "plt.xlabel('Types of time series')\n",
    "plt.ylabel('SMAPE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Trend type time series, the error distribution, and will the scale affect the error?\n",
    "\n",
    "- By plotting the scale (mean amplitude, consistent with the deepAR paper) vs the corresponding SMAPE, we see a correlation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(ans['trend'], bins = 20, label = 'trend')\n",
    "plt.xlabel('SMAPE')\n",
    "plt.ylabel('# of time series')\n",
    "plt.title('SMAPE distribution for Trend time series')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "trend_mean = []\n",
    "trend_error = []\n",
    "bad_index = []\n",
    "good_index = []\n",
    "for index, error in enumerate(error_list):\n",
    "    if (character_list[index][5] == 'trend'):\n",
    "        trend_mean.append(character_list[index][6])\n",
    "        trend_error.append(error_list[index])\n",
    "        if (error < 0.15 and character_list[index][6] > 1000):\n",
    "            good_index.append(index)\n",
    "        elif (error > 0.5 and character_list[index][6] > 1000):\n",
    "            bad_index.append(index) \n",
    "\n",
    "plt.figure()\n",
    "plt.plot((trend_mean), trend_error, '.')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('log of the scale (mean amplitude)')\n",
    "plt.ylabel('SMAPE error')\n",
    "plt.title('Examine the robustness over scales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in good_index[:2]:\n",
    "    plt.figure()\n",
    "    time_series_wiki[k][-prediction_length-context_length*2:].plot(label='target')\n",
    "    p10 = list_of_wiki_pred[k][0]['0.1']\n",
    "    p90 = list_of_wiki_pred[k][0]['0.9']\n",
    "    plt.fill_between(p10.index, p10, p90, color='y', alpha=0.5, label='80% confidence interval')\n",
    "    list_of_wiki_pred[k][0]['0.5'].plot(label='prediction median')\n",
    "    plt.title('Trend time series with SMAPE < 0.15')\n",
    "    plt.legend(loc = 'best')\n",
    "    plt.show() \n",
    "    print('[trend score, half day score, day score, week score, month score, strongest characteristic, time mean, time std]')\n",
    "    print(character_list[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in bad_index[:2]:\n",
    "    plt.figure()\n",
    "    time_series_wiki[k][-prediction_length-context_length*2:].plot(label='target')\n",
    "    p10 = list_of_wiki_pred[k][0]['0.1']\n",
    "    p90 = list_of_wiki_pred[k][0]['0.9']\n",
    "    plt.fill_between(p10.index, p10, p90, color='y', alpha=0.5, label='80% confidence interval')\n",
    "    list_of_wiki_pred[k][0]['0.5'].plot(label='prediction median')\n",
    "    plt.title('Day seasonality time series with SMAPE > 0.5')\n",
    "    plt.legend(loc = 'best')\n",
    "    plt.show()\n",
    "    print('[trend score, half day score, day score, week score, month score, strongest characteristic, time mean, time std]')\n",
    "    print(character_list[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the Day seasonality type time series, the error distribution, and will the scale affect the error?\n",
    "\n",
    "- Similar to the trend type, we see a correlation between the scale and the SMAPE. The model performs better for larger scales (i.e. time series for popular wiki projects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(ans['Day'], bins = 20, label = 'day')\n",
    "plt.xlabel('SMAPE')\n",
    "plt.ylabel('# of time series')\n",
    "plt.title('SMAPE distribution for Day seasonality time series')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "day_mean = []\n",
    "day_error = []\n",
    "bad_index = []\n",
    "good_index = []\n",
    "for index, error in enumerate(error_list):\n",
    "    if (character_list[index][5] == 'Day'):\n",
    "        day_mean.append(character_list[index][6])\n",
    "        day_error.append(error_list[index])\n",
    "        if error < 0.15:\n",
    "            good_index.append(index)\n",
    "        elif error > 0.4:\n",
    "            bad_index.append(index)    \n",
    "\n",
    "plt.figure()\n",
    "plt.plot((day_mean), day_error, '.')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('log of the scale (mean amplitude)')\n",
    "plt.ylabel('SMAPE error')\n",
    "plt.title('Examine the robustness over scales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in good_index[:2]:\n",
    "    plt.figure()\n",
    "    time_series_wiki[k][-prediction_length-context_length*2:].plot(label='target')\n",
    "    p10 = list_of_wiki_pred[k][0]['0.1']\n",
    "    p90 = list_of_wiki_pred[k][0]['0.9']\n",
    "    plt.fill_between(p10.index, p10, p90, color='y', alpha=0.5, label='80% confidence interval')\n",
    "    list_of_wiki_pred[k][0]['0.5'].plot(label='prediction median')\n",
    "    plt.title('Day seasonality time series with SMAPE < 0.15')\n",
    "    plt.legend(loc = 'best')\n",
    "    plt.show()    \n",
    "    print('[trend score, half day score, day score, week score, month score, strongest characteristic, time mean, time std]')\n",
    "    print(character_list[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in bad_index[:2]:\n",
    "    plt.figure()\n",
    "    time_series_wiki[k][-prediction_length-context_length*2:].plot(label='target')\n",
    "    p10 = list_of_wiki_pred[k][0]['0.1']\n",
    "    p90 = list_of_wiki_pred[k][0]['0.9']\n",
    "    plt.fill_between(p10.index, p10, p90, color='y', alpha=0.5, label='80% confidence interval')\n",
    "    list_of_wiki_pred[k][0]['0.5'].plot(label='prediction median')\n",
    "    plt.title('Day seasonality time series with SMAPE > 0.4')\n",
    "    plt.legend(loc = 'best')\n",
    "    plt.show()    \n",
    "    print('[trend score, half day score, day score, week score, month score, strongest characteristic, time mean, time std]')\n",
    "    print(character_list[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine DayImpulse type time series, why it has such high error?\n",
    "\n",
    "Find the ones with high SMAPE, and see how the actual and prediction compare to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(ans['DayImpulse'], label = 'DayImpulse')\n",
    "plt.xlabel('SMAPE')\n",
    "plt.ylabel('# of time series')\n",
    "plt.title('SMAPE distribution for DayImpulse time series')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "bad_index = []\n",
    "good_index = []\n",
    "for index, error in enumerate(error_list):\n",
    "    if (character_list[index][5] == 'DayImpulse' and error > 0.6):\n",
    "        bad_index.append(index)\n",
    "    elif (character_list[index][5] == 'DayImpulse' and error < 0.25):\n",
    "        good_index.append(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we take a look at the good and bad examples: we plot shorter length (only keep the context + prediction) to better zoom in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in bad_index[:2]:\n",
    "    plt.figure()\n",
    "    time_series_wiki[k][-prediction_length-context_length:].plot(label='target')\n",
    "    p10 = list_of_wiki_pred[k][0]['0.1']\n",
    "    p90 = list_of_wiki_pred[k][0]['0.9']\n",
    "    plt.fill_between(p10.index, p10, p90, color='y', alpha=0.5, label='80% confidence interval')\n",
    "    list_of_wiki_pred[k][0]['0.5'].plot(label='prediction median')\n",
    "    plt.title('DayImpulse time series with SMAPE > 0.6')\n",
    "    plt.legend(loc = 'best')\n",
    "    plt.show()    \n",
    "    print('[trend score, half day score, day score, week score, month score, strongest characteristic, time mean, time std]')\n",
    "    print(character_list[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in good_index:\n",
    "    plt.figure()\n",
    "    time_series_wiki[k][-prediction_length-context_length:].plot(label='target')\n",
    "    p10 = list_of_wiki_pred[k][0]['0.1']\n",
    "    p90 = list_of_wiki_pred[k][0]['0.9']\n",
    "    plt.fill_between(p10.index, p10, p90, color='y', alpha=0.5, label='80% confidence interval')\n",
    "    list_of_wiki_pred[k][0]['0.5'].plot(label='prediction median')\n",
    "    plt.title('DayImpulse time series with SMAPE < 0.25')\n",
    "    plt.legend(loc = 'best')\n",
    "    plt.show()   \n",
    "    print('[trend score, half day score, day score, week score, month score, strongest characteristic, time mean, time std]')\n",
    "    print(character_list[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.delete_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
