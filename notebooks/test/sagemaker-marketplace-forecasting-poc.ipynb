{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training forecasting models on traces from Marketplace\n",
    "This notebooks trains a forecasting models on traces from Marketplace with DeepAR on AWS SageMaker\n",
    "It follows the example [here](../sagemaker-memory-poc/sagemaker-memory-prediction-poc.ipynb)\n",
    "\n",
    "The data comes from scraping Marketplace metrics from Datadog\n",
    "\n",
    "This notebook will download the data needed, train a model, and test it. Just run all the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'manifoldco-sagemaker'\n",
    "prefix = 'hlnr-o'\n",
    "\n",
    "S3_PATH = os.path.join('s3://', bucket, prefix)\n",
    "\n",
    "MODEL_PATHS = {\n",
    "    'memory-forecasting': os.path.join(S3_PATH, 'memory-forecasting', 'marketplace-poc'),\n",
    "    'cpu-forecasting': os.path.join(S3_PATH, 'cpu-forecasting', 'marketplace-poc'),\n",
    "    'http-latency-forecasting': os.path.join(S3_PATH, 'http-latency-forecasting', 'marketplace-poc'),\n",
    "    'http-request-count-forecasting': os.path.join(S3_PATH, 'http-request-count-forecasting', 'marketplace-poc'),\n",
    "}\n",
    "\n",
    "DEEPAR_IMAGE = '522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = '5min'\n",
    "context_length = 30\n",
    "prediction_length = 30\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: memory-forecasting-marketplace-poc-2018-08-23-17-40-23-472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-23 17:40:23 Starting - Starting the training job............\n",
      "2018-08-23 17:42:09 Downloading - Downloading input data\n",
      "2018-08-23 17:42:16 Training - Training in-progress.........\n",
      "\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:47 INFO 140093389563712] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:47 INFO 140093389563712] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'dropout_rate': u'0.05', u'learning_rate': u'0.001', u'num_cells': u'32', u'prediction_length': u'30', u'epochs': u'20', u'time_freq': u'5min', u'context_length': u'30', u'num_layers': u'2', u'mini_batch_size': u'32', u'likelihood': u'student-t', u'early_stopping_patience': u'10'}\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:47 INFO 140093389563712] Final configuration: {u'dropout_rate': u'0.05', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_layers': u'2', u'epochs': u'20', u'embedding_dimension': u'10', u'num_cells': u'32', u'_num_kv_servers': u'auto', u'mini_batch_size': u'32', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'30', u'time_freq': u'5min', u'context_length': u'30', u'_kvstore': u'auto', u'early_stopping_patience': u'10'}\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:47 INFO 140093389563712] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:47 INFO 140093389563712] Using early stopping with patience 10\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:47 INFO 140093389563712] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/memory_query_75.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:47 INFO 140093389563712] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/memory_query_75.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:47 INFO 140093389563712] Training set statistics:\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:47 INFO 140093389563712] Real time series\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:47 INFO 140093389563712] number of time series: 19\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:47 INFO 140093389563712] number of observations: 250268\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:47 INFO 140093389563712] mean target length: 13172\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:47 INFO 140093389563712] min/mean/max target: 6336512.0/98060610.1246/1053175808.0\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:47 INFO 140093389563712] mean abs(target): 98060610.1246\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:47 INFO 140093389563712] contains missing values: no\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:47 INFO 140093389563712] Small number of time series. Doing 10 number of passes over dataset per epoch.\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:47 INFO 140093389563712] No test channel found not running evaluations\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:47 INFO 140093389563712] nvidia-smi took: 0.0251009464264 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:47 INFO 140093389563712] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:47 INFO 140093389563712] Create Store: local\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 430.7360649108887, \"sum\": 430.7360649108887, \"min\": 430.7360649108887}}, \"EndTime\": 1535046228.196944, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046227.763489}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:48 INFO 140093389563712] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 718.6172008514404, \"sum\": 718.6172008514404, \"min\": 718.6172008514404}}, \"EndTime\": 1535046228.482171, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046228.197044}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:49 INFO 140093389563712] Epoch[0] Batch[0] avg_epoch_loss=18.425159\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:49 INFO 140093389563712] Epoch[0] Batch[5] avg_epoch_loss=18.036601\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:49 INFO 140093389563712] Epoch[0] Batch [5]#011Speed: 487.12 samples/sec#011loss=18.036601\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:49 INFO 140093389563712] processed a total of 311 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}, \"update.time\": {\"count\": 1, \"max\": 1195.404052734375, \"sum\": 1195.404052734375, \"min\": 1195.404052734375}}, \"EndTime\": 1535046229.677695, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046228.482224}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:49 INFO 140093389563712] #throughput_metric: host=algo-1, train throughput=260.141340369 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:49 INFO 140093389563712] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:49 INFO 140093389563712] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:49 INFO 140093389563712] Saved checkpoint to \"/opt/ml/model/state_e26e41c3-8ab5-4ce1-8d99-4bac2199044f-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.709918975830078, \"sum\": 21.709918975830078, \"min\": 21.709918975830078}}, \"EndTime\": 1535046229.699789, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046229.677768}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:50 INFO 140093389563712] Epoch[1] Batch[0] avg_epoch_loss=17.707811\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:50 INFO 140093389563712] Epoch[1] Batch[5] avg_epoch_loss=16.970009\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:50 INFO 140093389563712] Epoch[1] Batch [5]#011Speed: 487.49 samples/sec#011loss=16.970009\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:50 INFO 140093389563712] processed a total of 310 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1120.3231811523438, \"sum\": 1120.3231811523438, \"min\": 1120.3231811523438}}, \"EndTime\": 1535046230.820201, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046229.699837}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:50 INFO 140093389563712] #throughput_metric: host=algo-1, train throughput=276.68320012 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:50 INFO 140093389563712] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:50 INFO 140093389563712] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:50 INFO 140093389563712] Saved checkpoint to \"/opt/ml/model/state_760079cb-cad6-421e-afeb-85e3a86ba637-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 26.213884353637695, \"sum\": 26.213884353637695, \"min\": 26.213884353637695}}, \"EndTime\": 1535046230.846744, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046230.820267}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:51 INFO 140093389563712] Epoch[2] Batch[0] avg_epoch_loss=16.245874\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:51 INFO 140093389563712] Epoch[2] Batch[5] avg_epoch_loss=16.291116\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:51 INFO 140093389563712] Epoch[2] Batch [5]#011Speed: 504.51 samples/sec#011loss=16.291116\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:52 INFO 140093389563712] Epoch[2] Batch[10] avg_epoch_loss=16.166803\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:52 INFO 140093389563712] Epoch[2] Batch [10]#011Speed: 489.85 samples/sec#011loss=16.017628\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:52 INFO 140093389563712] processed a total of 335 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1190.9170150756836, \"sum\": 1190.9170150756836, \"min\": 1190.9170150756836}}, \"EndTime\": 1535046232.037757, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046230.846798}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:52 INFO 140093389563712] #throughput_metric: host=algo-1, train throughput=281.275059619 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:52 INFO 140093389563712] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:52 INFO 140093389563712] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:52 INFO 140093389563712] Saved checkpoint to \"/opt/ml/model/state_f0327127-3f22-4eca-a423-363c19531060-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 26.324987411499023, \"sum\": 26.324987411499023, \"min\": 26.324987411499023}}, \"EndTime\": 1535046232.06441, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046232.037817}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:52 INFO 140093389563712] Epoch[3] Batch[0] avg_epoch_loss=15.905992\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:52 INFO 140093389563712] Epoch[3] Batch[5] avg_epoch_loss=15.613125\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:52 INFO 140093389563712] Epoch[3] Batch [5]#011Speed: 479.12 samples/sec#011loss=15.613125\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:53 INFO 140093389563712] Epoch[3] Batch[10] avg_epoch_loss=15.624439\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:53 INFO 140093389563712] Epoch[3] Batch [10]#011Speed: 480.93 samples/sec#011loss=15.638017\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:53 INFO 140093389563712] processed a total of 335 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1229.8228740692139, \"sum\": 1229.8228740692139, \"min\": 1229.8228740692139}}, \"EndTime\": 1535046233.294325, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046232.064462}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:53 INFO 140093389563712] #throughput_metric: host=algo-1, train throughput=272.380316545 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:53 INFO 140093389563712] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:53 INFO 140093389563712] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:53 INFO 140093389563712] Saved checkpoint to \"/opt/ml/model/state_54399c84-5e61-4704-a48a-d0fd3a785c40-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.52903175354004, \"sum\": 20.52903175354004, \"min\": 20.52903175354004}}, \"EndTime\": 1535046233.315137, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046233.294376}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:53 INFO 140093389563712] Epoch[4] Batch[0] avg_epoch_loss=15.814325\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:54 INFO 140093389563712] Epoch[4] Batch[5] avg_epoch_loss=15.377800\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:54 INFO 140093389563712] Epoch[4] Batch [5]#011Speed: 468.82 samples/sec#011loss=15.377800\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:54 INFO 140093389563712] processed a total of 304 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1113.3289337158203, \"sum\": 1113.3289337158203, \"min\": 1113.3289337158203}}, \"EndTime\": 1535046234.428554, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046233.315183}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:54 INFO 140093389563712] #throughput_metric: host=algo-1, train throughput=273.033103541 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:54 INFO 140093389563712] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:54 INFO 140093389563712] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:54 INFO 140093389563712] Saved checkpoint to \"/opt/ml/model/state_a4fe4c11-d928-4e40-8e6a-2e397eec6b67-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.80988883972168, \"sum\": 20.80988883972168, \"min\": 20.80988883972168}}, \"EndTime\": 1535046234.449688, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046234.428616}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:54 INFO 140093389563712] Epoch[5] Batch[0] avg_epoch_loss=15.312632\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:55 INFO 140093389563712] Epoch[5] Batch[5] avg_epoch_loss=14.993293\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:55 INFO 140093389563712] Epoch[5] Batch [5]#011Speed: 486.90 samples/sec#011loss=14.993293\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:55 INFO 140093389563712] Epoch[5] Batch[10] avg_epoch_loss=14.948192\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:55 INFO 140093389563712] Epoch[5] Batch [10]#011Speed: 460.15 samples/sec#011loss=14.894071\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:55 INFO 140093389563712] processed a total of 337 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1212.0859622955322, \"sum\": 1212.0859622955322, \"min\": 1212.0859622955322}}, \"EndTime\": 1535046235.66187, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046234.44974}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:55 INFO 140093389563712] #throughput_metric: host=algo-1, train throughput=278.013562765 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:55 INFO 140093389563712] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:55 INFO 140093389563712] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:55 INFO 140093389563712] Saved checkpoint to \"/opt/ml/model/state_7c34bddb-244a-4366-9e7f-41d82f04ea82-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 26.432037353515625, \"sum\": 26.432037353515625, \"min\": 26.432037353515625}}, \"EndTime\": 1535046235.688618, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046235.661928}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:56 INFO 140093389563712] Epoch[6] Batch[0] avg_epoch_loss=14.816584\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:56 INFO 140093389563712] Epoch[6] Batch[5] avg_epoch_loss=14.790863\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:56 INFO 140093389563712] Epoch[6] Batch [5]#011Speed: 480.24 samples/sec#011loss=14.790863\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:56 INFO 140093389563712] processed a total of 294 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1114.6018505096436, \"sum\": 1114.6018505096436, \"min\": 1114.6018505096436}}, \"EndTime\": 1535046236.803318, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046235.688672}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:56 INFO 140093389563712] #throughput_metric: host=algo-1, train throughput=263.749992995 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:56 INFO 140093389563712] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:56 INFO 140093389563712] loss did not improve for 1 epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/23/2018 17:43:57 INFO 140093389563712] Epoch[7] Batch[0] avg_epoch_loss=15.370348\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:57 INFO 140093389563712] Epoch[7] Batch[5] avg_epoch_loss=15.077854\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:57 INFO 140093389563712] Epoch[7] Batch [5]#011Speed: 493.97 samples/sec#011loss=15.077854\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:57 INFO 140093389563712] Epoch[7] Batch[10] avg_epoch_loss=14.963686\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:57 INFO 140093389563712] Epoch[7] Batch [10]#011Speed: 471.03 samples/sec#011loss=14.826684\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:57 INFO 140093389563712] processed a total of 347 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1196.018934249878, \"sum\": 1196.018934249878, \"min\": 1196.018934249878}}, \"EndTime\": 1535046237.999625, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046236.80338}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:57 INFO 140093389563712] #throughput_metric: host=algo-1, train throughput=290.108309075 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:57 INFO 140093389563712] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:57 INFO 140093389563712] loss did not improve for 2 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:58 INFO 140093389563712] Epoch[8] Batch[0] avg_epoch_loss=14.975545\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:58 INFO 140093389563712] Epoch[8] Batch[5] avg_epoch_loss=14.592210\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:58 INFO 140093389563712] Epoch[8] Batch [5]#011Speed: 480.13 samples/sec#011loss=14.592210\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:59 INFO 140093389563712] processed a total of 314 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1154.7868251800537, \"sum\": 1154.7868251800537, \"min\": 1154.7868251800537}}, \"EndTime\": 1535046239.154686, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046237.999684}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:59 INFO 140093389563712] #throughput_metric: host=algo-1, train throughput=271.890887853 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:59 INFO 140093389563712] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:59 INFO 140093389563712] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:59 INFO 140093389563712] Saved checkpoint to \"/opt/ml/model/state_6751acea-a49c-4b30-ab17-a306ee670d16-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 26.479005813598633, \"sum\": 26.479005813598633, \"min\": 26.479005813598633}}, \"EndTime\": 1535046239.181482, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046239.154747}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:43:59 INFO 140093389563712] Epoch[9] Batch[0] avg_epoch_loss=13.826655\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:00 INFO 140093389563712] Epoch[9] Batch[5] avg_epoch_loss=14.273955\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:00 INFO 140093389563712] Epoch[9] Batch [5]#011Speed: 494.79 samples/sec#011loss=14.273955\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:00 INFO 140093389563712] Epoch[9] Batch[10] avg_epoch_loss=14.315876\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:00 INFO 140093389563712] Epoch[9] Batch [10]#011Speed: 499.62 samples/sec#011loss=14.366181\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:00 INFO 140093389563712] processed a total of 339 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1168.734073638916, \"sum\": 1168.734073638916, \"min\": 1168.734073638916}}, \"EndTime\": 1535046240.350311, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046239.181534}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:00 INFO 140093389563712] #throughput_metric: host=algo-1, train throughput=290.036838479 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:00 INFO 140093389563712] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:00 INFO 140093389563712] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:00 INFO 140093389563712] Saved checkpoint to \"/opt/ml/model/state_5b4a3ccf-c812-43fc-9f35-7164fe6f991a-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.06509017944336, \"sum\": 23.06509017944336, \"min\": 23.06509017944336}}, \"EndTime\": 1535046240.373671, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046240.350365}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:00 INFO 140093389563712] Epoch[10] Batch[0] avg_epoch_loss=14.772465\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:01 INFO 140093389563712] Epoch[10] Batch[5] avg_epoch_loss=14.472953\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:01 INFO 140093389563712] Epoch[10] Batch [5]#011Speed: 489.80 samples/sec#011loss=14.472953\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:01 INFO 140093389563712] Epoch[10] Batch[10] avg_epoch_loss=14.467301\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:01 INFO 140093389563712] Epoch[10] Batch [10]#011Speed: 492.43 samples/sec#011loss=14.460519\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:01 INFO 140093389563712] processed a total of 332 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1184.3771934509277, \"sum\": 1184.3771934509277, \"min\": 1184.3771934509277}}, \"EndTime\": 1535046241.558132, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046240.373715}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:01 INFO 140093389563712] #throughput_metric: host=algo-1, train throughput=280.295339418 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:01 INFO 140093389563712] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:01 INFO 140093389563712] loss did not improve for 1 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:02 INFO 140093389563712] Epoch[11] Batch[0] avg_epoch_loss=14.463179\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:02 INFO 140093389563712] Epoch[11] Batch[5] avg_epoch_loss=14.365932\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:02 INFO 140093389563712] Epoch[11] Batch [5]#011Speed: 490.01 samples/sec#011loss=14.365932\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:02 INFO 140093389563712] Epoch[11] Batch[10] avg_epoch_loss=14.480806\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:02 INFO 140093389563712] Epoch[11] Batch [10]#011Speed: 510.58 samples/sec#011loss=14.618655\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:02 INFO 140093389563712] processed a total of 353 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1265.1751041412354, \"sum\": 1265.1751041412354, \"min\": 1265.1751041412354}}, \"EndTime\": 1535046242.823582, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046241.558193}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:02 INFO 140093389563712] #throughput_metric: host=algo-1, train throughput=278.992940896 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:02 INFO 140093389563712] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:02 INFO 140093389563712] loss did not improve for 2 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:03 INFO 140093389563712] Epoch[12] Batch[0] avg_epoch_loss=16.717697\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:03 INFO 140093389563712] Epoch[12] Batch[5] avg_epoch_loss=16.203989\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:03 INFO 140093389563712] Epoch[12] Batch [5]#011Speed: 494.90 samples/sec#011loss=16.203989\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:03 INFO 140093389563712] processed a total of 306 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1079.4599056243896, \"sum\": 1079.4599056243896, \"min\": 1079.4599056243896}}, \"EndTime\": 1535046243.903335, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046242.823645}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:03 INFO 140093389563712] #throughput_metric: host=algo-1, train throughput=283.451743789 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:03 INFO 140093389563712] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:03 INFO 140093389563712] loss did not improve for 3 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:04 INFO 140093389563712] Epoch[13] Batch[0] avg_epoch_loss=16.020596\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:04 INFO 140093389563712] Epoch[13] Batch[5] avg_epoch_loss=15.588135\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:04 INFO 140093389563712] Epoch[13] Batch [5]#011Speed: 494.99 samples/sec#011loss=15.588135\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:05 INFO 140093389563712] Epoch[13] Batch[10] avg_epoch_loss=15.260571\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:05 INFO 140093389563712] Epoch[13] Batch [10]#011Speed: 504.90 samples/sec#011loss=14.867496\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:05 INFO 140093389563712] processed a total of 340 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1170.1300144195557, \"sum\": 1170.1300144195557, \"min\": 1170.1300144195557}}, \"EndTime\": 1535046245.073754, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046243.903397}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:05 INFO 140093389563712] #throughput_metric: host=algo-1, train throughput=290.544630847 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:05 INFO 140093389563712] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:05 INFO 140093389563712] loss did not improve for 4 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:05 INFO 140093389563712] Epoch[14] Batch[0] avg_epoch_loss=14.859957\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:05 INFO 140093389563712] Epoch[14] Batch[5] avg_epoch_loss=14.933056\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:05 INFO 140093389563712] Epoch[14] Batch [5]#011Speed: 502.81 samples/sec#011loss=14.933056\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:06 INFO 140093389563712] Epoch[14] Batch[10] avg_epoch_loss=14.988068\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:06 INFO 140093389563712] Epoch[14] Batch [10]#011Speed: 504.38 samples/sec#011loss=15.054083\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:06 INFO 140093389563712] processed a total of 340 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1185.3611469268799, \"sum\": 1185.3611469268799, \"min\": 1185.3611469268799}}, \"EndTime\": 1535046246.259396, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046245.073813}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:06 INFO 140093389563712] #throughput_metric: host=algo-1, train throughput=286.810897043 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:06 INFO 140093389563712] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:06 INFO 140093389563712] loss did not improve for 5 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:06 INFO 140093389563712] Epoch[15] Batch[0] avg_epoch_loss=14.707209\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:07 INFO 140093389563712] Epoch[15] Batch[5] avg_epoch_loss=15.011847\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:07 INFO 140093389563712] Epoch[15] Batch [5]#011Speed: 492.20 samples/sec#011loss=15.011847\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:07 INFO 140093389563712] Epoch[15] Batch[10] avg_epoch_loss=14.837256\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:07 INFO 140093389563712] Epoch[15] Batch [10]#011Speed: 475.44 samples/sec#011loss=14.627748\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:07 INFO 140093389563712] processed a total of 343 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1182.5239658355713, \"sum\": 1182.5239658355713, \"min\": 1182.5239658355713}}, \"EndTime\": 1535046247.442191, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046246.259456}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:07 INFO 140093389563712] #throughput_metric: host=algo-1, train throughput=290.036202354 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:07 INFO 140093389563712] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:07 INFO 140093389563712] loss did not improve for 6 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:07 INFO 140093389563712] Epoch[16] Batch[0] avg_epoch_loss=14.006417\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:08 INFO 140093389563712] Epoch[16] Batch[5] avg_epoch_loss=14.452557\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:08 INFO 140093389563712] Epoch[16] Batch [5]#011Speed: 479.17 samples/sec#011loss=14.452557\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:08 INFO 140093389563712] processed a total of 294 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1103.3811569213867, \"sum\": 1103.3811569213867, \"min\": 1103.3811569213867}}, \"EndTime\": 1535046248.545848, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046247.442251}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:08 INFO 140093389563712] #throughput_metric: host=algo-1, train throughput=266.431773069 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:08 INFO 140093389563712] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:08 INFO 140093389563712] loss did not improve for 7 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:09 INFO 140093389563712] Epoch[17] Batch[0] avg_epoch_loss=14.642266\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:09 INFO 140093389563712] Epoch[17] Batch[5] avg_epoch_loss=14.570369\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:09 INFO 140093389563712] Epoch[17] Batch [5]#011Speed: 493.99 samples/sec#011loss=14.570369\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:09 INFO 140093389563712] Epoch[17] Batch[10] avg_epoch_loss=14.652981\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:09 INFO 140093389563712] Epoch[17] Batch [10]#011Speed: 476.52 samples/sec#011loss=14.752116\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:09 INFO 140093389563712] processed a total of 329 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1209.3048095703125, \"sum\": 1209.3048095703125, \"min\": 1209.3048095703125}}, \"EndTime\": 1535046249.755472, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046248.545912}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:09 INFO 140093389563712] #throughput_metric: host=algo-1, train throughput=272.037289244 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:09 INFO 140093389563712] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:09 INFO 140093389563712] loss did not improve for 8 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:10 INFO 140093389563712] Epoch[18] Batch[0] avg_epoch_loss=14.507383\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:10 INFO 140093389563712] Epoch[18] Batch[5] avg_epoch_loss=14.697289\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:10 INFO 140093389563712] Epoch[18] Batch [5]#011Speed: 580.81 samples/sec#011loss=14.697289\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:10 INFO 140093389563712] processed a total of 314 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1021.2259292602539, \"sum\": 1021.2259292602539, \"min\": 1021.2259292602539}}, \"EndTime\": 1535046250.776976, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046249.755532}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:10 INFO 140093389563712] #throughput_metric: host=algo-1, train throughput=307.448321585 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:10 INFO 140093389563712] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:10 INFO 140093389563712] loss did not improve for 9 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:11 INFO 140093389563712] Epoch[19] Batch[0] avg_epoch_loss=14.309311\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:11 INFO 140093389563712] Epoch[19] Batch[5] avg_epoch_loss=14.307392\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:11 INFO 140093389563712] Epoch[19] Batch [5]#011Speed: 583.38 samples/sec#011loss=14.307392\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:11 INFO 140093389563712] processed a total of 289 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 992.9628372192383, \"sum\": 992.9628372192383, \"min\": 992.9628372192383}}, \"EndTime\": 1535046251.770223, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046250.777032}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:11 INFO 140093389563712] #throughput_metric: host=algo-1, train throughput=291.02614173 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:11 INFO 140093389563712] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:11 INFO 140093389563712] loss did not improve for 10 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:11 INFO 140093389563712] stopping training now\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:11 INFO 140093389563712] #progress_metric: host=algo-1, completed 100 % epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:11 INFO 140093389563712] Loading parameters from best epoch (9)\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.deserialize.time\": {\"count\": 1, \"max\": 8.727073669433594, \"sum\": 8.727073669433594, \"min\": 8.727073669433594}}, \"EndTime\": 1535046251.779306, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046251.770274}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:11 INFO 140093389563712] Final loss: 14.315875747 (occurred at epoch 9)\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:11 INFO 140093389563712] #quality_metric: host=algo-1, train final_loss <loss>=14.315875747\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:11 INFO 140093389563712] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:11 WARNING 140093389563712] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:11 INFO 140093389563712] All workers finished. Serializing model for prediction.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 1223.228931427002, \"sum\": 1223.228931427002, \"min\": 1223.228931427002}}, \"EndTime\": 1535046253.002928, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046251.779358}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:13 INFO 140093389563712] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 2191.711902618408, \"sum\": 2191.711902618408, \"min\": 2191.711902618408}}, \"EndTime\": 1535046253.971383, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046253.003048}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:13 INFO 140093389563712] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:14 INFO 140093389563712] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 159.75689888000488, \"sum\": 159.75689888000488, \"min\": 159.75689888000488}}, \"EndTime\": 1535046254.13122, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046253.971431}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:14 INFO 140093389563712] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:44:14 INFO 140093389563712] No test data passed, skipping evaluation.\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 26773.92292022705, \"sum\": 26773.92292022705, \"min\": 26773.92292022705}, \"setuptime\": {\"count\": 1, \"max\": 7.339954376220703, \"sum\": 7.339954376220703, \"min\": 7.339954376220703}}, \"EndTime\": 1535046254.377537, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046254.131266}\n",
      "\u001b[0m\n",
      "\n",
      "2018-08-23 17:44:17 Uploading - Uploading generated training model\n",
      "2018-08-23 17:44:23 Completed - Training job completed\n",
      "Billable seconds: 134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: cpu-forecasting-marketplace-poc-2018-08-23-17-44-47-593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-23 17:44:48 Starting - Starting the training job............\n",
      "2018-08-23 17:46:51 Downloading - Downloading input data...\n",
      "2018-08-23 17:46:58 Training - Training in-progress..........\n",
      "\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:56 INFO 140653950183232] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:56 INFO 140653950183232] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'dropout_rate': u'0.05', u'learning_rate': u'0.001', u'num_cells': u'32', u'prediction_length': u'30', u'epochs': u'20', u'time_freq': u'5min', u'context_length': u'30', u'num_layers': u'2', u'mini_batch_size': u'32', u'likelihood': u'student-t', u'early_stopping_patience': u'10'}\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:56 INFO 140653950183232] Final configuration: {u'dropout_rate': u'0.05', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_layers': u'2', u'epochs': u'20', u'embedding_dimension': u'10', u'num_cells': u'32', u'_num_kv_servers': u'auto', u'mini_batch_size': u'32', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'30', u'time_freq': u'5min', u'context_length': u'30', u'_kvstore': u'auto', u'early_stopping_patience': u'10'}\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:56 INFO 140653950183232] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:56 INFO 140653950183232] Using early stopping with patience 10\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:56 INFO 140653950183232] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/cpu_query_75.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:56 INFO 140653950183232] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/cpu_query_75.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:56 INFO 140653950183232] Training set statistics:\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:56 INFO 140653950183232] Real time series\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:56 INFO 140653950183232] number of time series: 19\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:56 INFO 140653950183232] number of observations: 246240\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:56 INFO 140653950183232] mean target length: 12960\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:56 INFO 140653950183232] min/mean/max target: 66517.921875/103715391.453/2571421184.0\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:56 INFO 140653950183232] mean abs(target): 103715391.453\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:56 INFO 140653950183232] contains missing values: no\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:56 INFO 140653950183232] Small number of time series. Doing 10 number of passes over dataset per epoch.\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:56 INFO 140653950183232] No test channel found not running evaluations\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:56 INFO 140653950183232] nvidia-smi took: 0.0251159667969 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:56 INFO 140653950183232] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:56 INFO 140653950183232] Create Store: local\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 428.8630485534668, \"sum\": 428.8630485534668, \"min\": 428.8630485534668}}, \"EndTime\": 1535046536.847063, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046536.415374}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:56 INFO 140653950183232] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 714.8010730743408, \"sum\": 714.8010730743408, \"min\": 714.8010730743408}}, \"EndTime\": 1535046537.130252, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046536.847159}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:57 INFO 140653950183232] Epoch[0] Batch[0] avg_epoch_loss=16.160151\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:58 INFO 140653950183232] Epoch[0] Batch[5] avg_epoch_loss=15.785089\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:58 INFO 140653950183232] Epoch[0] Batch [5]#011Speed: 554.99 samples/sec#011loss=15.785089\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:58 INFO 140653950183232] Epoch[0] Batch[10] avg_epoch_loss=15.877616\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:58 INFO 140653950183232] Epoch[0] Batch [10]#011Speed: 520.14 samples/sec#011loss=15.988650\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:58 INFO 140653950183232] processed a total of 338 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}, \"update.time\": {\"count\": 1, \"max\": 1266.880989074707, \"sum\": 1266.880989074707, \"min\": 1266.880989074707}}, \"EndTime\": 1535046538.397245, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046537.130307}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:58 INFO 140653950183232] #throughput_metric: host=algo-1, train throughput=266.77698086 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:58 INFO 140653950183232] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:58 INFO 140653950183232] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:58 INFO 140653950183232] Saved checkpoint to \"/opt/ml/model/state_ca71bffb-8dc8-4b31-8ef0-71e465a2276e-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.712873458862305, \"sum\": 23.712873458862305, \"min\": 23.712873458862305}}, \"EndTime\": 1535046538.421294, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046538.397312}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:58 INFO 140653950183232] Epoch[1] Batch[0] avg_epoch_loss=15.418874\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:59 INFO 140653950183232] Epoch[1] Batch[5] avg_epoch_loss=15.228775\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:59 INFO 140653950183232] Epoch[1] Batch [5]#011Speed: 543.61 samples/sec#011loss=15.228775\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:59 INFO 140653950183232] processed a total of 312 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1048.8629341125488, \"sum\": 1048.8629341125488, \"min\": 1048.8629341125488}}, \"EndTime\": 1535046539.470247, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046538.421344}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:59 INFO 140653950183232] #throughput_metric: host=algo-1, train throughput=297.439430788 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:59 INFO 140653950183232] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:59 INFO 140653950183232] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:48:59 INFO 140653950183232] Saved checkpoint to \"/opt/ml/model/state_d9cd04d5-68cd-4a6a-93df-131b7f55ce4e-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 24.631023406982422, \"sum\": 24.631023406982422, \"min\": 24.631023406982422}}, \"EndTime\": 1535046539.495199, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046539.470309}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:00 INFO 140653950183232] Epoch[2] Batch[0] avg_epoch_loss=15.594883\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:00 INFO 140653950183232] Epoch[2] Batch[5] avg_epoch_loss=14.752516\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:00 INFO 140653950183232] Epoch[2] Batch [5]#011Speed: 542.75 samples/sec#011loss=14.752516\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:00 INFO 140653950183232] processed a total of 314 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1053.0931949615479, \"sum\": 1053.0931949615479, \"min\": 1053.0931949615479}}, \"EndTime\": 1535046540.548385, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046539.495248}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:00 INFO 140653950183232] #throughput_metric: host=algo-1, train throughput=298.144402506 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:00 INFO 140653950183232] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:00 INFO 140653950183232] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:00 INFO 140653950183232] Saved checkpoint to \"/opt/ml/model/state_b6a1fa33-9244-4986-8fd2-b1fde864d661-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 24.555206298828125, \"sum\": 24.555206298828125, \"min\": 24.555206298828125}}, \"EndTime\": 1535046540.573257, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046540.548444}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:01 INFO 140653950183232] Epoch[3] Batch[0] avg_epoch_loss=14.710916\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:01 INFO 140653950183232] Epoch[3] Batch[5] avg_epoch_loss=14.931636\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:01 INFO 140653950183232] Epoch[3] Batch [5]#011Speed: 524.30 samples/sec#011loss=14.931636\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:01 INFO 140653950183232] processed a total of 319 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1077.2759914398193, \"sum\": 1077.2759914398193, \"min\": 1077.2759914398193}}, \"EndTime\": 1535046541.650619, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046540.573303}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:01 INFO 140653950183232] #throughput_metric: host=algo-1, train throughput=296.092803481 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:01 INFO 140653950183232] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:01 INFO 140653950183232] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:01 INFO 140653950183232] Saved checkpoint to \"/opt/ml/model/state_34c615d6-721f-4f5c-aed2-04f2ba017f01-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 26.629924774169922, \"sum\": 26.629924774169922, \"min\": 26.629924774169922}}, \"EndTime\": 1535046541.677572, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046541.650681}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:02 INFO 140653950183232] Epoch[4] Batch[0] avg_epoch_loss=15.771177\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:02 INFO 140653950183232] Epoch[4] Batch[5] avg_epoch_loss=15.232840\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:02 INFO 140653950183232] Epoch[4] Batch [5]#011Speed: 535.70 samples/sec#011loss=15.232840\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:02 INFO 140653950183232] processed a total of 308 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1066.2360191345215, \"sum\": 1066.2360191345215, \"min\": 1066.2360191345215}}, \"EndTime\": 1535046542.743906, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046541.677625}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:02 INFO 140653950183232] #throughput_metric: host=algo-1, train throughput=288.841694049 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:02 INFO 140653950183232] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:02 INFO 140653950183232] loss did not improve for 1 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:03 INFO 140653950183232] Epoch[5] Batch[0] avg_epoch_loss=14.242939\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:03 INFO 140653950183232] Epoch[5] Batch[5] avg_epoch_loss=15.039277\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:03 INFO 140653950183232] Epoch[5] Batch [5]#011Speed: 553.02 samples/sec#011loss=15.039277\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/23/2018 17:49:03 INFO 140653950183232] processed a total of 320 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1073.5270977020264, \"sum\": 1073.5270977020264, \"min\": 1073.5270977020264}}, \"EndTime\": 1535046543.817727, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046542.743968}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:03 INFO 140653950183232] #throughput_metric: host=algo-1, train throughput=298.058143419 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:03 INFO 140653950183232] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:03 INFO 140653950183232] loss did not improve for 2 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:04 INFO 140653950183232] Epoch[6] Batch[0] avg_epoch_loss=14.985869\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:04 INFO 140653950183232] Epoch[6] Batch[5] avg_epoch_loss=14.760305\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:04 INFO 140653950183232] Epoch[6] Batch [5]#011Speed: 539.22 samples/sec#011loss=14.760305\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:04 INFO 140653950183232] Epoch[6] Batch[10] avg_epoch_loss=14.868924\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:04 INFO 140653950183232] Epoch[6] Batch [10]#011Speed: 498.92 samples/sec#011loss=14.999267\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:04 INFO 140653950183232] processed a total of 331 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1137.5699043273926, \"sum\": 1137.5699043273926, \"min\": 1137.5699043273926}}, \"EndTime\": 1535046544.955586, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046543.817787}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:04 INFO 140653950183232] #throughput_metric: host=algo-1, train throughput=290.949116191 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:04 INFO 140653950183232] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:04 INFO 140653950183232] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:04 INFO 140653950183232] Saved checkpoint to \"/opt/ml/model/state_d6101b23-7488-40ed-90d7-d42adc12df8b-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 25.913000106811523, \"sum\": 25.913000106811523, \"min\": 25.913000106811523}}, \"EndTime\": 1535046544.981802, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046544.955645}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:05 INFO 140653950183232] Epoch[7] Batch[0] avg_epoch_loss=14.433149\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:05 INFO 140653950183232] Epoch[7] Batch[5] avg_epoch_loss=14.699912\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:05 INFO 140653950183232] Epoch[7] Batch [5]#011Speed: 548.63 samples/sec#011loss=14.699912\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:06 INFO 140653950183232] processed a total of 298 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1023.1540203094482, \"sum\": 1023.1540203094482, \"min\": 1023.1540203094482}}, \"EndTime\": 1535046546.005053, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046544.981854}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:06 INFO 140653950183232] #throughput_metric: host=algo-1, train throughput=291.230323121 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:06 INFO 140653950183232] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:06 INFO 140653950183232] loss did not improve for 1 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:06 INFO 140653950183232] Epoch[8] Batch[0] avg_epoch_loss=14.976482\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:06 INFO 140653950183232] Epoch[8] Batch[5] avg_epoch_loss=14.668130\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:06 INFO 140653950183232] Epoch[8] Batch [5]#011Speed: 601.79 samples/sec#011loss=14.668130\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:06 INFO 140653950183232] processed a total of 313 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 985.6758117675781, \"sum\": 985.6758117675781, \"min\": 985.6758117675781}}, \"EndTime\": 1535046546.991018, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046546.005115}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:06 INFO 140653950183232] #throughput_metric: host=algo-1, train throughput=317.523434703 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:06 INFO 140653950183232] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:06 INFO 140653950183232] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:07 INFO 140653950183232] Saved checkpoint to \"/opt/ml/model/state_880ae922-c3fd-4861-bfd3-ef86e9b24d98-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.531892776489258, \"sum\": 20.531892776489258, \"min\": 20.531892776489258}}, \"EndTime\": 1535046547.011846, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046546.991069}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:07 INFO 140653950183232] Epoch[9] Batch[0] avg_epoch_loss=15.108239\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:07 INFO 140653950183232] Epoch[9] Batch[5] avg_epoch_loss=14.796860\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:07 INFO 140653950183232] Epoch[9] Batch [5]#011Speed: 565.24 samples/sec#011loss=14.796860\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:08 INFO 140653950183232] processed a total of 290 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1008.5790157318115, \"sum\": 1008.5790157318115, \"min\": 1008.5790157318115}}, \"EndTime\": 1535046548.020511, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046547.011893}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:08 INFO 140653950183232] #throughput_metric: host=algo-1, train throughput=287.507897113 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:08 INFO 140653950183232] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:08 INFO 140653950183232] loss did not improve for 1 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:08 INFO 140653950183232] Epoch[10] Batch[0] avg_epoch_loss=15.272783\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:08 INFO 140653950183232] Epoch[10] Batch[5] avg_epoch_loss=14.940764\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:08 INFO 140653950183232] Epoch[10] Batch [5]#011Speed: 544.19 samples/sec#011loss=14.940764\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:09 INFO 140653950183232] processed a total of 307 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1047.778844833374, \"sum\": 1047.778844833374, \"min\": 1047.778844833374}}, \"EndTime\": 1535046549.06858, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046548.020573}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:09 INFO 140653950183232] #throughput_metric: host=algo-1, train throughput=292.975562453 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:09 INFO 140653950183232] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:09 INFO 140653950183232] loss did not improve for 2 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:09 INFO 140653950183232] Epoch[11] Batch[0] avg_epoch_loss=14.754918\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:09 INFO 140653950183232] Epoch[11] Batch[5] avg_epoch_loss=14.662286\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:09 INFO 140653950183232] Epoch[11] Batch [5]#011Speed: 534.81 samples/sec#011loss=14.662286\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:10 INFO 140653950183232] Epoch[11] Batch[10] avg_epoch_loss=14.646352\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:10 INFO 140653950183232] Epoch[11] Batch [10]#011Speed: 527.33 samples/sec#011loss=14.627232\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:10 INFO 140653950183232] processed a total of 330 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1118.617057800293, \"sum\": 1118.617057800293, \"min\": 1118.617057800293}}, \"EndTime\": 1535046550.187489, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046549.068642}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:10 INFO 140653950183232] #throughput_metric: host=algo-1, train throughput=294.984489086 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:10 INFO 140653950183232] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:10 INFO 140653950183232] loss did not improve for 3 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:10 INFO 140653950183232] Epoch[12] Batch[0] avg_epoch_loss=14.930597\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:10 INFO 140653950183232] Epoch[12] Batch[5] avg_epoch_loss=14.942565\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:10 INFO 140653950183232] Epoch[12] Batch [5]#011Speed: 547.40 samples/sec#011loss=14.942565\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:11 INFO 140653950183232] processed a total of 300 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1046.861171722412, \"sum\": 1046.861171722412, \"min\": 1046.861171722412}}, \"EndTime\": 1535046551.234624, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046550.187548}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:11 INFO 140653950183232] #throughput_metric: host=algo-1, train throughput=286.54693386 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:11 INFO 140653950183232] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:11 INFO 140653950183232] loss did not improve for 4 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:11 INFO 140653950183232] Epoch[13] Batch[0] avg_epoch_loss=14.286046\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:12 INFO 140653950183232] Epoch[13] Batch[5] avg_epoch_loss=14.717236\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:12 INFO 140653950183232] Epoch[13] Batch [5]#011Speed: 544.51 samples/sec#011loss=14.717236\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:12 INFO 140653950183232] Epoch[13] Batch[10] avg_epoch_loss=14.851778\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:12 INFO 140653950183232] Epoch[13] Batch [10]#011Speed: 532.88 samples/sec#011loss=15.013228\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:12 INFO 140653950183232] processed a total of 334 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1121.9508647918701, \"sum\": 1121.9508647918701, \"min\": 1121.9508647918701}}, \"EndTime\": 1535046552.356863, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046551.234685}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:12 INFO 140653950183232] #throughput_metric: host=algo-1, train throughput=297.658616477 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:12 INFO 140653950183232] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:12 INFO 140653950183232] loss did not improve for 5 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:12 INFO 140653950183232] Epoch[14] Batch[0] avg_epoch_loss=14.404107\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:13 INFO 140653950183232] Epoch[14] Batch[5] avg_epoch_loss=14.666709\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:13 INFO 140653950183232] Epoch[14] Batch [5]#011Speed: 546.63 samples/sec#011loss=14.666709\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:13 INFO 140653950183232] Epoch[14] Batch[10] avg_epoch_loss=14.515091\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:13 INFO 140653950183232] Epoch[14] Batch [10]#011Speed: 527.80 samples/sec#011loss=14.333150\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:13 INFO 140653950183232] processed a total of 330 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1120.7091808319092, \"sum\": 1120.7091808319092, \"min\": 1120.7091808319092}}, \"EndTime\": 1535046553.477944, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046552.356971}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:13 INFO 140653950183232] #throughput_metric: host=algo-1, train throughput=294.433608121 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:13 INFO 140653950183232] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:13 INFO 140653950183232] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:13 INFO 140653950183232] Saved checkpoint to \"/opt/ml/model/state_494f95ac-04d1-4158-bfb1-cac86b247b85-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.997047424316406, \"sum\": 20.997047424316406, \"min\": 20.997047424316406}}, \"EndTime\": 1535046553.499247, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046553.478002}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2018-08-23 17:49:25 Uploading - Uploading generated training model\u001b[31m[08/23/2018 17:49:14 INFO 140653950183232] Epoch[15] Batch[0] avg_epoch_loss=14.995030\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:14 INFO 140653950183232] Epoch[15] Batch[5] avg_epoch_loss=14.919340\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:14 INFO 140653950183232] Epoch[15] Batch [5]#011Speed: 548.96 samples/sec#011loss=14.919340\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:14 INFO 140653950183232] processed a total of 317 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1044.6219444274902, \"sum\": 1044.6219444274902, \"min\": 1044.6219444274902}}, \"EndTime\": 1535046554.543962, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046553.499295}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:14 INFO 140653950183232] #throughput_metric: host=algo-1, train throughput=303.433511699 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:14 INFO 140653950183232] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:14 INFO 140653950183232] loss did not improve for 1 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:15 INFO 140653950183232] Epoch[16] Batch[0] avg_epoch_loss=15.630372\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:15 INFO 140653950183232] Epoch[16] Batch[5] avg_epoch_loss=14.892139\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:15 INFO 140653950183232] Epoch[16] Batch [5]#011Speed: 548.11 samples/sec#011loss=14.892139\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:15 INFO 140653950183232] Epoch[16] Batch[10] avg_epoch_loss=14.772677\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:15 INFO 140653950183232] Epoch[16] Batch [10]#011Speed: 513.97 samples/sec#011loss=14.629322\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:15 INFO 140653950183232] processed a total of 360 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1217.8089618682861, \"sum\": 1217.8089618682861, \"min\": 1217.8089618682861}}, \"EndTime\": 1535046555.762065, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046554.544022}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:15 INFO 140653950183232] #throughput_metric: host=algo-1, train throughput=295.591224758 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:15 INFO 140653950183232] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:15 INFO 140653950183232] loss did not improve for 2 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:16 INFO 140653950183232] Epoch[17] Batch[0] avg_epoch_loss=14.562582\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:16 INFO 140653950183232] Epoch[17] Batch[5] avg_epoch_loss=14.202343\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:16 INFO 140653950183232] Epoch[17] Batch [5]#011Speed: 541.60 samples/sec#011loss=14.202343\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:16 INFO 140653950183232] Epoch[17] Batch[10] avg_epoch_loss=14.330119\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:16 INFO 140653950183232] Epoch[17] Batch [10]#011Speed: 508.45 samples/sec#011loss=14.483451\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:16 INFO 140653950183232] processed a total of 324 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1120.434045791626, \"sum\": 1120.434045791626, \"min\": 1120.434045791626}}, \"EndTime\": 1535046556.882789, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046555.762125}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:16 INFO 140653950183232] #throughput_metric: host=algo-1, train throughput=289.151435741 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:16 INFO 140653950183232] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:16 INFO 140653950183232] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:16 INFO 140653950183232] Saved checkpoint to \"/opt/ml/model/state_68b145fd-904e-4069-9ccb-d4b7315c31f3-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.54397964477539, \"sum\": 21.54397964477539, \"min\": 21.54397964477539}}, \"EndTime\": 1535046556.904633, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046556.882848}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:17 INFO 140653950183232] Epoch[18] Batch[0] avg_epoch_loss=15.223302\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:17 INFO 140653950183232] Epoch[18] Batch[5] avg_epoch_loss=14.833397\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:17 INFO 140653950183232] Epoch[18] Batch [5]#011Speed: 536.21 samples/sec#011loss=14.833397\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:18 INFO 140653950183232] Epoch[18] Batch[10] avg_epoch_loss=14.701128\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:18 INFO 140653950183232] Epoch[18] Batch [10]#011Speed: 544.69 samples/sec#011loss=14.542406\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:18 INFO 140653950183232] processed a total of 337 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1118.5510158538818, \"sum\": 1118.5510158538818, \"min\": 1118.5510158538818}}, \"EndTime\": 1535046558.023273, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046556.904677}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:18 INFO 140653950183232] #throughput_metric: host=algo-1, train throughput=301.259713123 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:18 INFO 140653950183232] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:18 INFO 140653950183232] loss did not improve for 1 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:18 INFO 140653950183232] Epoch[19] Batch[0] avg_epoch_loss=14.684011\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:18 INFO 140653950183232] Epoch[19] Batch[5] avg_epoch_loss=14.736750\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:18 INFO 140653950183232] Epoch[19] Batch [5]#011Speed: 597.08 samples/sec#011loss=14.736750\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:19 INFO 140653950183232] Epoch[19] Batch[10] avg_epoch_loss=14.872034\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:19 INFO 140653950183232] Epoch[19] Batch [10]#011Speed: 566.76 samples/sec#011loss=15.034374\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:19 INFO 140653950183232] processed a total of 341 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1066.004991531372, \"sum\": 1066.004991531372, \"min\": 1066.004991531372}}, \"EndTime\": 1535046559.089544, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046558.02333}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:19 INFO 140653950183232] #throughput_metric: host=algo-1, train throughput=319.864326815 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:19 INFO 140653950183232] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:19 INFO 140653950183232] loss did not improve for 2 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:19 INFO 140653950183232] Loading parameters from best epoch (17)\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.deserialize.time\": {\"count\": 1, \"max\": 8.612871170043945, \"sum\": 8.612871170043945, \"min\": 8.612871170043945}}, \"EndTime\": 1535046559.098417, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046559.089593}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:19 INFO 140653950183232] Final loss: 14.3301190463 (occurred at epoch 17)\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:19 INFO 140653950183232] #quality_metric: host=algo-1, train final_loss <loss>=14.3301190463\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:19 INFO 140653950183232] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:19 WARNING 140653950183232] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:19 INFO 140653950183232] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 1205.5950164794922, \"sum\": 1205.5950164794922, \"min\": 1205.5950164794922}}, \"EndTime\": 1535046560.304426, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046559.098468}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:21 INFO 140653950183232] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 2146.404981613159, \"sum\": 2146.404981613159, \"min\": 2146.404981613159}}, \"EndTime\": 1535046561.245213, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046560.304535}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:21 INFO 140653950183232] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:21 INFO 140653950183232] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 156.10790252685547, \"sum\": 156.10790252685547, \"min\": 156.10790252685547}}, \"EndTime\": 1535046561.40141, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046561.245265}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:21 INFO 140653950183232] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:49:21 INFO 140653950183232] No test data passed, skipping evaluation.\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 25387.657165527344, \"sum\": 25387.657165527344, \"min\": 25387.657165527344}, \"setuptime\": {\"count\": 1, \"max\": 6.858110427856445, \"sum\": 6.858110427856445, \"min\": 6.858110427856445}}, \"EndTime\": 1535046561.645887, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046561.401455}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2018-08-23 17:49:31 Completed - Training job completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: http-latency-forecasting-marketplace-po-2018-08-23-17-50-12-874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Billable seconds: 160\n",
      "2018-08-23 17:50:13 Starting - Starting the training job............\n",
      "2018-08-23 17:51:53 Downloading - Downloading input data\n",
      "2018-08-23 17:52:00 Training - Training in-progress.........\n",
      "\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:34 INFO 140677364438848] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:34 INFO 140677364438848] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'dropout_rate': u'0.05', u'learning_rate': u'0.001', u'num_cells': u'32', u'prediction_length': u'30', u'epochs': u'20', u'time_freq': u'5min', u'context_length': u'30', u'num_layers': u'2', u'mini_batch_size': u'32', u'likelihood': u'student-t', u'early_stopping_patience': u'10'}\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:34 INFO 140677364438848] Final configuration: {u'dropout_rate': u'0.05', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_layers': u'2', u'epochs': u'20', u'embedding_dimension': u'10', u'num_cells': u'32', u'_num_kv_servers': u'auto', u'mini_batch_size': u'32', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'30', u'time_freq': u'5min', u'context_length': u'30', u'_kvstore': u'auto', u'early_stopping_patience': u'10'}\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:34 INFO 140677364438848] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:34 INFO 140677364438848] Using early stopping with patience 10\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:34 INFO 140677364438848] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/http_request_query_150.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:34 INFO 140677364438848] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/http_request_query_150.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:34 INFO 140677364438848] Training set statistics:\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:34 INFO 140677364438848] Real time series\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:34 INFO 140677364438848] number of time series: 15\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:34 INFO 140677364438848] number of observations: 224970\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:34 INFO 140677364438848] mean target length: 14998\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:34 INFO 140677364438848] min/mean/max target: 0.0/16.7037682281/1187.0\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:34 INFO 140677364438848] mean abs(target): 16.7037682281\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:34 INFO 140677364438848] contains missing values: no\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:34 INFO 140677364438848] Small number of time series. Doing 10 number of passes over dataset per epoch.\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:34 INFO 140677364438848] No test channel found not running evaluations\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:34 INFO 140677364438848] nvidia-smi took: 0.0251178741455 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:34 INFO 140677364438848] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:34 INFO 140677364438848] Create Store: local\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 429.83293533325195, \"sum\": 429.83293533325195, \"min\": 429.83293533325195}}, \"EndTime\": 1535046815.016505, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046814.583459}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:35 INFO 140677364438848] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 746.2499141693115, \"sum\": 746.2499141693115, \"min\": 746.2499141693115}}, \"EndTime\": 1535046815.329777, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046815.01661}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:35 INFO 140677364438848] Epoch[0] Batch[0] avg_epoch_loss=3.940846\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:36 INFO 140677364438848] Epoch[0] Batch[5] avg_epoch_loss=3.623363\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:36 INFO 140677364438848] Epoch[0] Batch [5]#011Speed: 472.08 samples/sec#011loss=3.623363\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:36 INFO 140677364438848] Epoch[0] Batch[10] avg_epoch_loss=3.419802\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:36 INFO 140677364438848] Epoch[0] Batch [10]#011Speed: 468.07 samples/sec#011loss=3.175529\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:36 INFO 140677364438848] processed a total of 347 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}, \"update.time\": {\"count\": 1, \"max\": 1285.325050354004, \"sum\": 1285.325050354004, \"min\": 1285.325050354004}}, \"EndTime\": 1535046816.61523, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046815.329839}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:36 INFO 140677364438848] #throughput_metric: host=algo-1, train throughput=269.948587068 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:36 INFO 140677364438848] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:36 INFO 140677364438848] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:36 INFO 140677364438848] Saved checkpoint to \"/opt/ml/model/state_fa6d1fd9-8b6b-40be-962f-fee997557a9f-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 27.35280990600586, \"sum\": 27.35280990600586, \"min\": 27.35280990600586}}, \"EndTime\": 1535046816.64294, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046816.615303}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:37 INFO 140677364438848] Epoch[1] Batch[0] avg_epoch_loss=3.102547\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:37 INFO 140677364438848] Epoch[1] Batch[5] avg_epoch_loss=3.143895\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:37 INFO 140677364438848] Epoch[1] Batch [5]#011Speed: 572.63 samples/sec#011loss=3.143895\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:37 INFO 140677364438848] processed a total of 312 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 945.6000328063965, \"sum\": 945.6000328063965, \"min\": 945.6000328063965}}, \"EndTime\": 1535046817.588647, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046816.643002}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:37 INFO 140677364438848] #throughput_metric: host=algo-1, train throughput=329.920944395 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:37 INFO 140677364438848] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:37 INFO 140677364438848] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:37 INFO 140677364438848] Saved checkpoint to \"/opt/ml/model/state_a0c14cd1-684c-4ddf-8010-1741c7c242a9-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.76101303100586, \"sum\": 20.76101303100586, \"min\": 20.76101303100586}}, \"EndTime\": 1535046817.609715, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046817.588701}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:38 INFO 140677364438848] Epoch[2] Batch[0] avg_epoch_loss=3.108399\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:38 INFO 140677364438848] Epoch[2] Batch[5] avg_epoch_loss=3.119552\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:38 INFO 140677364438848] Epoch[2] Batch [5]#011Speed: 570.12 samples/sec#011loss=3.119552\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:38 INFO 140677364438848] processed a total of 310 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 969.775915145874, \"sum\": 969.775915145874, \"min\": 969.775915145874}}, \"EndTime\": 1535046818.579582, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046817.609763}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:38 INFO 140677364438848] #throughput_metric: host=algo-1, train throughput=319.631457574 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:38 INFO 140677364438848] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:38 INFO 140677364438848] loss did not improve for 1 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:39 INFO 140677364438848] Epoch[3] Batch[0] avg_epoch_loss=3.307642\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:39 INFO 140677364438848] Epoch[3] Batch[5] avg_epoch_loss=3.083567\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:39 INFO 140677364438848] Epoch[3] Batch [5]#011Speed: 534.48 samples/sec#011loss=3.083567\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:39 INFO 140677364438848] Epoch[3] Batch[10] avg_epoch_loss=3.088206\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:39 INFO 140677364438848] Epoch[3] Batch [10]#011Speed: 437.06 samples/sec#011loss=3.093772\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:39 INFO 140677364438848] processed a total of 331 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1123.136043548584, \"sum\": 1123.136043548584, \"min\": 1123.136043548584}}, \"EndTime\": 1535046819.703037, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046818.579644}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:39 INFO 140677364438848] #throughput_metric: host=algo-1, train throughput=294.687180001 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:39 INFO 140677364438848] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:39 INFO 140677364438848] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:39 INFO 140677364438848] Saved checkpoint to \"/opt/ml/model/state_ca49ad54-58bd-4bb0-b138-d17c8d5239cd-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 26.74102783203125, \"sum\": 26.74102783203125, \"min\": 26.74102783203125}}, \"EndTime\": 1535046819.73009, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046819.703098}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:40 INFO 140677364438848] Epoch[4] Batch[0] avg_epoch_loss=3.372423\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:40 INFO 140677364438848] Epoch[4] Batch[5] avg_epoch_loss=3.064614\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:40 INFO 140677364438848] Epoch[4] Batch [5]#011Speed: 472.23 samples/sec#011loss=3.064614\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:40 INFO 140677364438848] processed a total of 319 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1053.7538528442383, \"sum\": 1053.7538528442383, \"min\": 1053.7538528442383}}, \"EndTime\": 1535046820.783948, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046819.730148}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:40 INFO 140677364438848] #throughput_metric: host=algo-1, train throughput=302.699643701 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:40 INFO 140677364438848] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:40 INFO 140677364438848] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:40 INFO 140677364438848] Saved checkpoint to \"/opt/ml/model/state_664cbcd9-9b43-423d-990d-557e631c6366-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 27.17900276184082, \"sum\": 27.17900276184082, \"min\": 27.17900276184082}}, \"EndTime\": 1535046820.811465, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046820.784015}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:41 INFO 140677364438848] Epoch[5] Batch[0] avg_epoch_loss=3.193452\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:41 INFO 140677364438848] Epoch[5] Batch[5] avg_epoch_loss=3.285503\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:41 INFO 140677364438848] Epoch[5] Batch [5]#011Speed: 440.89 samples/sec#011loss=3.285503\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:41 INFO 140677364438848] processed a total of 299 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1112.7829551696777, \"sum\": 1112.7829551696777, \"min\": 1112.7829551696777}}, \"EndTime\": 1535046821.924352, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046820.811524}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:41 INFO 140677364438848] #throughput_metric: host=algo-1, train throughput=268.676132305 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:41 INFO 140677364438848] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:41 INFO 140677364438848] loss did not improve for 1 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:42 INFO 140677364438848] Epoch[6] Batch[0] avg_epoch_loss=3.050143\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:42 INFO 140677364438848] Epoch[6] Batch[5] avg_epoch_loss=2.891252\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:42 INFO 140677364438848] Epoch[6] Batch [5]#011Speed: 445.17 samples/sec#011loss=2.891252\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:43 INFO 140677364438848] processed a total of 315 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1115.8831119537354, \"sum\": 1115.8831119537354, \"min\": 1115.8831119537354}}, \"EndTime\": 1535046823.040517, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046821.924406}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:43 INFO 140677364438848] #throughput_metric: host=algo-1, train throughput=282.266944916 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:43 INFO 140677364438848] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:43 INFO 140677364438848] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:43 INFO 140677364438848] Saved checkpoint to \"/opt/ml/model/state_053d54ff-aa53-4f10-8ab6-77a56f4eae5d-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.999908447265625, \"sum\": 20.999908447265625, \"min\": 20.999908447265625}}, \"EndTime\": 1535046823.061805, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046823.040575}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:43 INFO 140677364438848] Epoch[7] Batch[0] avg_epoch_loss=3.141947\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:43 INFO 140677364438848] Epoch[7] Batch[5] avg_epoch_loss=3.033713\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:43 INFO 140677364438848] Epoch[7] Batch [5]#011Speed: 473.50 samples/sec#011loss=3.033713\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:44 INFO 140677364438848] processed a total of 304 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1020.3890800476074, \"sum\": 1020.3890800476074, \"min\": 1020.3890800476074}}, \"EndTime\": 1535046824.082286, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046823.061856}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:44 INFO 140677364438848] #throughput_metric: host=algo-1, train throughput=297.902531989 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:44 INFO 140677364438848] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:44 INFO 140677364438848] loss did not improve for 1 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:44 INFO 140677364438848] Epoch[8] Batch[0] avg_epoch_loss=2.792417\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:44 INFO 140677364438848] Epoch[8] Batch[5] avg_epoch_loss=2.952901\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:44 INFO 140677364438848] Epoch[8] Batch [5]#011Speed: 564.59 samples/sec#011loss=2.952901\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:45 INFO 140677364438848] Epoch[8] Batch[10] avg_epoch_loss=2.936971\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:45 INFO 140677364438848] Epoch[8] Batch [10]#011Speed: 495.82 samples/sec#011loss=2.917855\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:45 INFO 140677364438848] processed a total of 375 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1169.684886932373, \"sum\": 1169.684886932373, \"min\": 1169.684886932373}}, \"EndTime\": 1535046825.252234, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046824.08234}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:45 INFO 140677364438848] #throughput_metric: host=algo-1, train throughput=320.572833495 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:45 INFO 140677364438848] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:45 INFO 140677364438848] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:45 INFO 140677364438848] Saved checkpoint to \"/opt/ml/model/state_d6a81cda-4ff9-46c7-a3a5-18cde900f675-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 28.779983520507812, \"sum\": 28.779983520507812, \"min\": 28.779983520507812}}, \"EndTime\": 1535046825.281358, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046825.252301}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:45 INFO 140677364438848] Epoch[9] Batch[0] avg_epoch_loss=2.567444\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:46 INFO 140677364438848] Epoch[9] Batch[5] avg_epoch_loss=2.846379\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:46 INFO 140677364438848] Epoch[9] Batch [5]#011Speed: 472.53 samples/sec#011loss=2.846379\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:46 INFO 140677364438848] processed a total of 298 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1110.727071762085, \"sum\": 1110.727071762085, \"min\": 1110.727071762085}}, \"EndTime\": 1535046826.392196, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046825.281422}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:46 INFO 140677364438848] #throughput_metric: host=algo-1, train throughput=268.271022068 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:46 INFO 140677364438848] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:46 INFO 140677364438848] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:46 INFO 140677364438848] Saved checkpoint to \"/opt/ml/model/state_1a95d672-c462-4539-bffe-dd6a40ca9d84-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 26.461124420166016, \"sum\": 26.461124420166016, \"min\": 26.461124420166016}}, \"EndTime\": 1535046826.418982, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046826.392259}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:46 INFO 140677364438848] Epoch[10] Batch[0] avg_epoch_loss=3.015184\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:47 INFO 140677364438848] Epoch[10] Batch[5] avg_epoch_loss=3.053110\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:47 INFO 140677364438848] Epoch[10] Batch [5]#011Speed: 472.98 samples/sec#011loss=3.053110\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:47 INFO 140677364438848] processed a total of 290 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1048.279047012329, \"sum\": 1048.279047012329, \"min\": 1048.279047012329}}, \"EndTime\": 1535046827.467365, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046826.419039}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:47 INFO 140677364438848] #throughput_metric: host=algo-1, train throughput=276.61935995 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:47 INFO 140677364438848] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:47 INFO 140677364438848] loss did not improve for 1 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:47 INFO 140677364438848] Epoch[11] Batch[0] avg_epoch_loss=3.054899\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:48 INFO 140677364438848] Epoch[11] Batch[5] avg_epoch_loss=2.902178\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:48 INFO 140677364438848] Epoch[11] Batch [5]#011Speed: 474.32 samples/sec#011loss=2.902178\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:48 INFO 140677364438848] Epoch[11] Batch[10] avg_epoch_loss=2.948486\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:48 INFO 140677364438848] Epoch[11] Batch [10]#011Speed: 436.84 samples/sec#011loss=3.004056\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:48 INFO 140677364438848] processed a total of 331 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1178.5240173339844, \"sum\": 1178.5240173339844, \"min\": 1178.5240173339844}}, \"EndTime\": 1535046828.646193, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046827.467429}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:48 INFO 140677364438848] #throughput_metric: host=algo-1, train throughput=280.838534614 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:48 INFO 140677364438848] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:48 INFO 140677364438848] loss did not improve for 2 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:49 INFO 140677364438848] Epoch[12] Batch[0] avg_epoch_loss=3.300201\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:49 INFO 140677364438848] Epoch[12] Batch[5] avg_epoch_loss=2.987048\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:49 INFO 140677364438848] Epoch[12] Batch [5]#011Speed: 490.59 samples/sec#011loss=2.987048\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:49 INFO 140677364438848] Epoch[12] Batch[10] avg_epoch_loss=2.904763\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:49 INFO 140677364438848] Epoch[12] Batch [10]#011Speed: 453.03 samples/sec#011loss=2.806021\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:49 INFO 140677364438848] processed a total of 330 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1142.2760486602783, \"sum\": 1142.2760486602783, \"min\": 1142.2760486602783}}, \"EndTime\": 1535046829.788745, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046828.646254}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:49 INFO 140677364438848] #throughput_metric: host=algo-1, train throughput=288.874160741 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:49 INFO 140677364438848] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:49 INFO 140677364438848] loss did not improve for 3 epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/23/2018 17:53:50 INFO 140677364438848] Epoch[13] Batch[0] avg_epoch_loss=2.844733\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:50 INFO 140677364438848] Epoch[13] Batch[5] avg_epoch_loss=2.850773\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:50 INFO 140677364438848] Epoch[13] Batch [5]#011Speed: 494.53 samples/sec#011loss=2.850773\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:50 INFO 140677364438848] processed a total of 280 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 935.5719089508057, \"sum\": 935.5719089508057, \"min\": 935.5719089508057}}, \"EndTime\": 1535046830.724603, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046829.788808}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:50 INFO 140677364438848] #throughput_metric: host=algo-1, train throughput=299.252056584 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:50 INFO 140677364438848] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:50 INFO 140677364438848] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:50 INFO 140677364438848] Saved checkpoint to \"/opt/ml/model/state_54e53de9-29d4-4d70-ab16-ac00e7f52a72-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 27.248859405517578, \"sum\": 27.248859405517578, \"min\": 27.248859405517578}}, \"EndTime\": 1535046830.752187, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046830.724667}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:51 INFO 140677364438848] Epoch[14] Batch[0] avg_epoch_loss=2.804350\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:51 INFO 140677364438848] Epoch[14] Batch[5] avg_epoch_loss=2.871036\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:51 INFO 140677364438848] Epoch[14] Batch [5]#011Speed: 560.45 samples/sec#011loss=2.871036\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:51 INFO 140677364438848] processed a total of 308 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 990.3140068054199, \"sum\": 990.3140068054199, \"min\": 990.3140068054199}}, \"EndTime\": 1535046831.74261, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046830.752248}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:51 INFO 140677364438848] #throughput_metric: host=algo-1, train throughput=310.98356501 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:51 INFO 140677364438848] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:51 INFO 140677364438848] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:51 INFO 140677364438848] Saved checkpoint to \"/opt/ml/model/state_d3d3a418-56ff-47f5-932a-eaf903d56df2-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 25.0699520111084, \"sum\": 25.0699520111084, \"min\": 25.0699520111084}}, \"EndTime\": 1535046831.768011, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046831.742673}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:52 INFO 140677364438848] Epoch[15] Batch[0] avg_epoch_loss=3.070500\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:52 INFO 140677364438848] Epoch[15] Batch[5] avg_epoch_loss=2.830870\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:52 INFO 140677364438848] Epoch[15] Batch [5]#011Speed: 491.67 samples/sec#011loss=2.830870\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:52 INFO 140677364438848] processed a total of 272 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 950.1159191131592, \"sum\": 950.1159191131592, \"min\": 950.1159191131592}}, \"EndTime\": 1535046832.718225, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046831.768064}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:52 INFO 140677364438848] #throughput_metric: host=algo-1, train throughput=286.251909558 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:52 INFO 140677364438848] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:52 INFO 140677364438848] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:52 INFO 140677364438848] Saved checkpoint to \"/opt/ml/model/state_c2914b65-eb5a-48ae-94a6-6b293732db1c-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 25.448083877563477, \"sum\": 25.448083877563477, \"min\": 25.448083877563477}}, \"EndTime\": 1535046832.744001, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046832.71829}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:53 INFO 140677364438848] Epoch[16] Batch[0] avg_epoch_loss=2.940048\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:53 INFO 140677364438848] Epoch[16] Batch[5] avg_epoch_loss=2.920488\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:53 INFO 140677364438848] Epoch[16] Batch [5]#011Speed: 491.35 samples/sec#011loss=2.920488\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:53 INFO 140677364438848] Epoch[16] Batch[10] avg_epoch_loss=2.930009\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:53 INFO 140677364438848] Epoch[16] Batch [10]#011Speed: 463.47 samples/sec#011loss=2.941434\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:53 INFO 140677364438848] processed a total of 326 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1121.9980716705322, \"sum\": 1121.9980716705322, \"min\": 1121.9980716705322}}, \"EndTime\": 1535046833.866094, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046832.744053}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:53 INFO 140677364438848] #throughput_metric: host=algo-1, train throughput=290.530056472 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:53 INFO 140677364438848] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:53 INFO 140677364438848] loss did not improve for 1 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:54 INFO 140677364438848] Epoch[17] Batch[0] avg_epoch_loss=2.673669\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:54 INFO 140677364438848] Epoch[17] Batch[5] avg_epoch_loss=2.967739\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:54 INFO 140677364438848] Epoch[17] Batch [5]#011Speed: 490.47 samples/sec#011loss=2.967739\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:55 INFO 140677364438848] Epoch[17] Batch[10] avg_epoch_loss=2.932405\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:55 INFO 140677364438848] Epoch[17] Batch [10]#011Speed: 463.21 samples/sec#011loss=2.890005\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:55 INFO 140677364438848] processed a total of 323 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1179.6650886535645, \"sum\": 1179.6650886535645, \"min\": 1179.6650886535645}}, \"EndTime\": 1535046835.046039, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046833.866156}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:55 INFO 140677364438848] #throughput_metric: host=algo-1, train throughput=273.786329238 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:55 INFO 140677364438848] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:55 INFO 140677364438848] loss did not improve for 2 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:55 INFO 140677364438848] Epoch[18] Batch[0] avg_epoch_loss=2.790253\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:55 INFO 140677364438848] Epoch[18] Batch[5] avg_epoch_loss=2.807057\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:55 INFO 140677364438848] Epoch[18] Batch [5]#011Speed: 484.22 samples/sec#011loss=2.807057\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:56 INFO 140677364438848] processed a total of 302 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1067.1191215515137, \"sum\": 1067.1191215515137, \"min\": 1067.1191215515137}}, \"EndTime\": 1535046836.113436, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046835.046098}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:56 INFO 140677364438848] #throughput_metric: host=algo-1, train throughput=282.981120366 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:56 INFO 140677364438848] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:56 INFO 140677364438848] loss did not improve for 3 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:56 INFO 140677364438848] Epoch[19] Batch[0] avg_epoch_loss=3.207287\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:56 INFO 140677364438848] Epoch[19] Batch[5] avg_epoch_loss=2.936272\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:56 INFO 140677364438848] Epoch[19] Batch [5]#011Speed: 488.25 samples/sec#011loss=2.936272\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:57 INFO 140677364438848] Epoch[19] Batch[10] avg_epoch_loss=3.154903\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:57 INFO 140677364438848] Epoch[19] Batch [10]#011Speed: 447.73 samples/sec#011loss=3.417261\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:57 INFO 140677364438848] processed a total of 321 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1143.5019969940186, \"sum\": 1143.5019969940186, \"min\": 1143.5019969940186}}, \"EndTime\": 1535046837.257233, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046836.113497}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:57 INFO 140677364438848] #throughput_metric: host=algo-1, train throughput=280.694074528 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:57 INFO 140677364438848] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:57 INFO 140677364438848] loss did not improve for 4 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:57 INFO 140677364438848] Loading parameters from best epoch (15)\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.deserialize.time\": {\"count\": 1, \"max\": 10.23101806640625, \"sum\": 10.23101806640625, \"min\": 10.23101806640625}}, \"EndTime\": 1535046837.267796, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046837.257296}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:57 INFO 140677364438848] Final loss: 2.80236890581 (occurred at epoch 15)\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:57 INFO 140677364438848] #quality_metric: host=algo-1, train final_loss <loss>=2.80236890581\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:57 INFO 140677364438848] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:57 WARNING 140677364438848] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:57 INFO 140677364438848] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 1235.8050346374512, \"sum\": 1235.8050346374512, \"min\": 1235.8050346374512}}, \"EndTime\": 1535046838.504038, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046837.267851}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:59 INFO 140677364438848] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 2344.4929122924805, \"sum\": 2344.4929122924805, \"min\": 2344.4929122924805}}, \"EndTime\": 1535046839.612694, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046838.504169}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:59 INFO 140677364438848] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:59 INFO 140677364438848] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 167.8009033203125, \"sum\": 167.8009033203125, \"min\": 167.8009033203125}}, \"EndTime\": 1535046839.780579, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046839.612745}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:59 INFO 140677364438848] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:53:59 INFO 140677364438848] No test data passed, skipping evaluation.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 25606.745958328247, \"sum\": 25606.745958328247, \"min\": 25606.745958328247}, \"setuptime\": {\"count\": 1, \"max\": 6.903886795043945, \"sum\": 6.903886795043945, \"min\": 6.903886795043945}}, \"EndTime\": 1535046840.043929, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535046839.780626}\n",
      "\u001b[0m\n",
      "\n",
      "2018-08-23 17:54:04 Uploading - Uploading generated training model\n",
      "2018-08-23 17:54:09 Completed - Training job completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: http-request-count-forecasting-marketpl-2018-08-23-17-54-34-508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Billable seconds: 136\n",
      "2018-08-23 17:54:34 Starting - Starting the training job............\n",
      "2018-08-23 17:56:13 Downloading - Downloading input data\n",
      "2018-08-23 17:56:19 Training - Training in-progress........\n",
      "\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:53 INFO 140477245183808] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:53 INFO 140477245183808] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'dropout_rate': u'0.05', u'learning_rate': u'0.001', u'num_cells': u'32', u'prediction_length': u'30', u'epochs': u'20', u'time_freq': u'5min', u'context_length': u'30', u'num_layers': u'2', u'mini_batch_size': u'32', u'likelihood': u'student-t', u'early_stopping_patience': u'10'}\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:53 INFO 140477245183808] Final configuration: {u'dropout_rate': u'0.05', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_layers': u'2', u'epochs': u'20', u'embedding_dimension': u'10', u'num_cells': u'32', u'_num_kv_servers': u'auto', u'mini_batch_size': u'32', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'30', u'time_freq': u'5min', u'context_length': u'30', u'_kvstore': u'auto', u'early_stopping_patience': u'10'}\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:53 INFO 140477245183808] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:53 INFO 140477245183808] Using early stopping with patience 10\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:53 INFO 140477245183808] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/http_count_query_150.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:53 INFO 140477245183808] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/http_count_query_150.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:53 INFO 140477245183808] Training set statistics:\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:53 INFO 140477245183808] Integer time series\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:53 INFO 140477245183808] number of time series: 15\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:53 INFO 140477245183808] number of observations: 224970\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:53 INFO 140477245183808] mean target length: 14998\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:53 INFO 140477245183808] min/mean/max target: 0.0/13.2577677024/119.0\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:53 INFO 140477245183808] mean abs(target): 13.2577677024\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:53 INFO 140477245183808] contains missing values: yes (0.0%)\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:53 INFO 140477245183808] Small number of time series. Doing 10 number of passes over dataset per epoch.\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:53 INFO 140477245183808] No test channel found not running evaluations\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:53 INFO 140477245183808] nvidia-smi took: 0.0251078605652 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:53 INFO 140477245183808] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:53 INFO 140477245183808] Create Store: local\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 430.61208724975586, \"sum\": 430.61208724975586, \"min\": 430.61208724975586}}, \"EndTime\": 1535047074.249632, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047073.816378}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:54 INFO 140477245183808] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 713.8710021972656, \"sum\": 713.8710021972656, \"min\": 713.8710021972656}}, \"EndTime\": 1535047074.530315, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047074.24973}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:55 INFO 140477245183808] Epoch[0] Batch[0] avg_epoch_loss=5.137437\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:55 INFO 140477245183808] Epoch[0] Batch[5] avg_epoch_loss=4.274712\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:55 INFO 140477245183808] Epoch[0] Batch [5]#011Speed: 552.16 samples/sec#011loss=4.274712\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:55 INFO 140477245183808] processed a total of 311 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}, \"update.time\": {\"count\": 1, \"max\": 1124.6700286865234, \"sum\": 1124.6700286865234, \"min\": 1124.6700286865234}}, \"EndTime\": 1535047075.655102, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047074.530373}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:55 INFO 140477245183808] #throughput_metric: host=algo-1, train throughput=276.499995337 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:55 INFO 140477245183808] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:55 INFO 140477245183808] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:55 INFO 140477245183808] Saved checkpoint to \"/opt/ml/model/state_7eef91c7-e801-4564-8482-7f9a92f7d23a-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 26.322126388549805, \"sum\": 26.322126388549805, \"min\": 26.322126388549805}}, \"EndTime\": 1535047075.6818, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047075.655179}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:56 INFO 140477245183808] Epoch[1] Batch[0] avg_epoch_loss=2.767062\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:56 INFO 140477245183808] Epoch[1] Batch[5] avg_epoch_loss=2.785443\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:56 INFO 140477245183808] Epoch[1] Batch [5]#011Speed: 478.36 samples/sec#011loss=2.785443\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:56 INFO 140477245183808] Epoch[1] Batch[10] avg_epoch_loss=2.679551\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:56 INFO 140477245183808] Epoch[1] Batch [10]#011Speed: 434.79 samples/sec#011loss=2.552481\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:56 INFO 140477245183808] processed a total of 323 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1198.5371112823486, \"sum\": 1198.5371112823486, \"min\": 1198.5371112823486}}, \"EndTime\": 1535047076.880432, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047075.681854}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:56 INFO 140477245183808] #throughput_metric: host=algo-1, train throughput=269.47633211 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:56 INFO 140477245183808] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:56 INFO 140477245183808] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:56 INFO 140477245183808] Saved checkpoint to \"/opt/ml/model/state_a13e61f1-2474-47cf-b4c6-b57458fa6f80-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 24.646997451782227, \"sum\": 24.646997451782227, \"min\": 24.646997451782227}}, \"EndTime\": 1535047076.905381, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047076.880491}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:57 INFO 140477245183808] Epoch[2] Batch[0] avg_epoch_loss=2.254963\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:57 INFO 140477245183808] Epoch[2] Batch[5] avg_epoch_loss=2.157437\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:57 INFO 140477245183808] Epoch[2] Batch [5]#011Speed: 497.93 samples/sec#011loss=2.157437\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:58 INFO 140477245183808] Epoch[2] Batch[10] avg_epoch_loss=2.029745\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:58 INFO 140477245183808] Epoch[2] Batch [10]#011Speed: 455.10 samples/sec#011loss=1.876515\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:58 INFO 140477245183808] processed a total of 347 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1190.4640197753906, \"sum\": 1190.4640197753906, \"min\": 1190.4640197753906}}, \"EndTime\": 1535047078.095943, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047076.905433}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:58 INFO 140477245183808] #throughput_metric: host=algo-1, train throughput=291.461440658 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:58 INFO 140477245183808] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:58 INFO 140477245183808] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:58 INFO 140477245183808] Saved checkpoint to \"/opt/ml/model/state_9812be46-2cb7-4920-9318-0942b4205c8e-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 26.268959045410156, \"sum\": 26.268959045410156, \"min\": 26.268959045410156}}, \"EndTime\": 1535047078.122548, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047078.096003}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:58 INFO 140477245183808] Epoch[3] Batch[0] avg_epoch_loss=1.963280\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:58 INFO 140477245183808] Epoch[3] Batch[5] avg_epoch_loss=1.739563\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:58 INFO 140477245183808] Epoch[3] Batch [5]#011Speed: 568.99 samples/sec#011loss=1.739563\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:59 INFO 140477245183808] processed a total of 316 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1036.9889736175537, \"sum\": 1036.9889736175537, \"min\": 1036.9889736175537}}, \"EndTime\": 1535047079.159634, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047078.122602}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:59 INFO 140477245183808] #throughput_metric: host=algo-1, train throughput=304.705500562 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:59 INFO 140477245183808] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:59 INFO 140477245183808] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:59 INFO 140477245183808] Saved checkpoint to \"/opt/ml/model/state_0faa1da4-93fb-47f2-abcc-7fdf910ddd83-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.486831665039062, \"sum\": 20.486831665039062, \"min\": 20.486831665039062}}, \"EndTime\": 1535047079.180389, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047079.159687}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:59 INFO 140477245183808] Epoch[4] Batch[0] avg_epoch_loss=2.080103\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:59 INFO 140477245183808] Epoch[4] Batch[5] avg_epoch_loss=1.719109\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:57:59 INFO 140477245183808] Epoch[4] Batch [5]#011Speed: 589.83 samples/sec#011loss=1.719109\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:00 INFO 140477245183808] Epoch[4] Batch[10] avg_epoch_loss=1.643529\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:00 INFO 140477245183808] Epoch[4] Batch [10]#011Speed: 446.62 samples/sec#011loss=1.552833\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:00 INFO 140477245183808] processed a total of 326 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1120.2619075775146, \"sum\": 1120.2619075775146, \"min\": 1120.2619075775146}}, \"EndTime\": 1535047080.300732, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047079.180434}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:00 INFO 140477245183808] #throughput_metric: host=algo-1, train throughput=290.983623457 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:00 INFO 140477245183808] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:00 INFO 140477245183808] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:00 INFO 140477245183808] Saved checkpoint to \"/opt/ml/model/state_d505273b-7e5a-4a5b-9f0e-03257b3218eb-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.212100982666016, \"sum\": 21.212100982666016, \"min\": 21.212100982666016}}, \"EndTime\": 1535047080.322225, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047080.300784}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:00 INFO 140477245183808] Epoch[5] Batch[0] avg_epoch_loss=1.728591\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:01 INFO 140477245183808] Epoch[5] Batch[5] avg_epoch_loss=1.649758\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:01 INFO 140477245183808] Epoch[5] Batch [5]#011Speed: 451.65 samples/sec#011loss=1.649758\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:01 INFO 140477245183808] processed a total of 314 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1130.2199363708496, \"sum\": 1130.2199363708496, \"min\": 1130.2199363708496}}, \"EndTime\": 1535047081.452531, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047080.322271}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:01 INFO 140477245183808] #throughput_metric: host=algo-1, train throughput=277.802869677 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:01 INFO 140477245183808] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:01 INFO 140477245183808] loss did not improve for 1 epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/23/2018 17:58:01 INFO 140477245183808] Epoch[6] Batch[0] avg_epoch_loss=1.526797\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:02 INFO 140477245183808] Epoch[6] Batch[5] avg_epoch_loss=1.470686\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:02 INFO 140477245183808] Epoch[6] Batch [5]#011Speed: 459.39 samples/sec#011loss=1.470686\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:02 INFO 140477245183808] processed a total of 312 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1127.4418830871582, \"sum\": 1127.4418830871582, \"min\": 1127.4418830871582}}, \"EndTime\": 1535047082.580252, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047081.452583}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:02 INFO 140477245183808] #throughput_metric: host=algo-1, train throughput=276.707623304 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:02 INFO 140477245183808] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:02 INFO 140477245183808] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:02 INFO 140477245183808] Saved checkpoint to \"/opt/ml/model/state_8bee63f0-1ee8-4f15-9a51-19f55cf055ee-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 24.871826171875, \"sum\": 24.871826171875, \"min\": 24.871826171875}}, \"EndTime\": 1535047082.605516, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047082.580324}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:03 INFO 140477245183808] Epoch[7] Batch[0] avg_epoch_loss=1.440634\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:03 INFO 140477245183808] Epoch[7] Batch[5] avg_epoch_loss=1.340747\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:03 INFO 140477245183808] Epoch[7] Batch [5]#011Speed: 479.81 samples/sec#011loss=1.340747\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:03 INFO 140477245183808] processed a total of 305 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1056.5881729125977, \"sum\": 1056.5881729125977, \"min\": 1056.5881729125977}}, \"EndTime\": 1535047083.662186, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047082.605559}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:03 INFO 140477245183808] #throughput_metric: host=algo-1, train throughput=288.640747433 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:03 INFO 140477245183808] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:03 INFO 140477245183808] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:03 INFO 140477245183808] Saved checkpoint to \"/opt/ml/model/state_1ed38bbc-0b11-460b-a2b9-6a07106fe091-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 26.221036911010742, \"sum\": 26.221036911010742, \"min\": 26.221036911010742}}, \"EndTime\": 1535047083.688726, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047083.662248}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:04 INFO 140477245183808] Epoch[8] Batch[0] avg_epoch_loss=1.428234\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:04 INFO 140477245183808] Epoch[8] Batch[5] avg_epoch_loss=1.214240\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:04 INFO 140477245183808] Epoch[8] Batch [5]#011Speed: 577.69 samples/sec#011loss=1.214240\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:04 INFO 140477245183808] Epoch[8] Batch[10] avg_epoch_loss=1.183945\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:04 INFO 140477245183808] Epoch[8] Batch [10]#011Speed: 446.72 samples/sec#011loss=1.147591\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:04 INFO 140477245183808] processed a total of 332 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1143.6569690704346, \"sum\": 1143.6569690704346, \"min\": 1143.6569690704346}}, \"EndTime\": 1535047084.832479, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047083.688779}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:04 INFO 140477245183808] #throughput_metric: host=algo-1, train throughput=290.276322109 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:04 INFO 140477245183808] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:04 INFO 140477245183808] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:04 INFO 140477245183808] Saved checkpoint to \"/opt/ml/model/state_e4acc5c4-6258-4285-88f0-911663bb4b81-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.718097686767578, \"sum\": 20.718097686767578, \"min\": 20.718097686767578}}, \"EndTime\": 1535047084.853477, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047084.832535}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:05 INFO 140477245183808] Epoch[9] Batch[0] avg_epoch_loss=1.421287\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:05 INFO 140477245183808] Epoch[9] Batch[5] avg_epoch_loss=1.439443\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:05 INFO 140477245183808] Epoch[9] Batch [5]#011Speed: 467.37 samples/sec#011loss=1.439443\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:05 INFO 140477245183808] processed a total of 291 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1108.9200973510742, \"sum\": 1108.9200973510742, \"min\": 1108.9200973510742}}, \"EndTime\": 1535047085.962481, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047084.853521}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:05 INFO 140477245183808] #throughput_metric: host=algo-1, train throughput=262.397380288 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:05 INFO 140477245183808] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:05 INFO 140477245183808] loss did not improve for 1 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:06 INFO 140477245183808] Epoch[10] Batch[0] avg_epoch_loss=0.880937\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:06 INFO 140477245183808] Epoch[10] Batch[5] avg_epoch_loss=1.035444\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:06 INFO 140477245183808] Epoch[10] Batch [5]#011Speed: 454.36 samples/sec#011loss=1.035444\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:07 INFO 140477245183808] processed a total of 314 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1126.30295753479, \"sum\": 1126.30295753479, \"min\": 1126.30295753479}}, \"EndTime\": 1535047087.089083, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047085.962542}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:07 INFO 140477245183808] #throughput_metric: host=algo-1, train throughput=278.768395432 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:07 INFO 140477245183808] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:07 INFO 140477245183808] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:07 INFO 140477245183808] Saved checkpoint to \"/opt/ml/model/state_8bdd1d89-4f3a-4dbe-ad58-7b5021adf014-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.111083984375, \"sum\": 20.111083984375, \"min\": 20.111083984375}}, \"EndTime\": 1535047087.109548, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047087.089139}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:07 INFO 140477245183808] Epoch[11] Batch[0] avg_epoch_loss=1.253739\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:07 INFO 140477245183808] Epoch[11] Batch[5] avg_epoch_loss=1.171295\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:07 INFO 140477245183808] Epoch[11] Batch [5]#011Speed: 462.38 samples/sec#011loss=1.171295\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:08 INFO 140477245183808] processed a total of 303 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1093.0709838867188, \"sum\": 1093.0709838867188, \"min\": 1093.0709838867188}}, \"EndTime\": 1535047088.202708, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047087.109596}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:08 INFO 140477245183808] #throughput_metric: host=algo-1, train throughput=277.178050558 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:08 INFO 140477245183808] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:08 INFO 140477245183808] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:08 INFO 140477245183808] Saved checkpoint to \"/opt/ml/model/state_3647b961-89eb-44bf-852d-62b7e4c8e26d-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 24.26314353942871, \"sum\": 24.26314353942871, \"min\": 24.26314353942871}}, \"EndTime\": 1535047088.227292, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047088.20277}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:08 INFO 140477245183808] Epoch[12] Batch[0] avg_epoch_loss=1.225175\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:09 INFO 140477245183808] Epoch[12] Batch[5] avg_epoch_loss=1.042663\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:09 INFO 140477245183808] Epoch[12] Batch [5]#011Speed: 568.66 samples/sec#011loss=1.042663\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:09 INFO 140477245183808] Epoch[12] Batch[10] avg_epoch_loss=1.087124\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:09 INFO 140477245183808] Epoch[12] Batch [10]#011Speed: 548.50 samples/sec#011loss=1.140477\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:09 INFO 140477245183808] processed a total of 323 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1078.9179801940918, \"sum\": 1078.9179801940918, \"min\": 1078.9179801940918}}, \"EndTime\": 1535047089.306299, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047088.227339}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:09 INFO 140477245183808] #throughput_metric: host=algo-1, train throughput=299.351516799 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:09 INFO 140477245183808] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:09 INFO 140477245183808] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:09 INFO 140477245183808] Saved checkpoint to \"/opt/ml/model/state_4d0ca192-8927-4a9d-a0fe-bc7bbb165de8-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.752906799316406, \"sum\": 20.752906799316406, \"min\": 20.752906799316406}}, \"EndTime\": 1535047089.327335, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047089.306353}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:09 INFO 140477245183808] Epoch[13] Batch[0] avg_epoch_loss=1.213195\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:10 INFO 140477245183808] Epoch[13] Batch[5] avg_epoch_loss=0.979532\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:10 INFO 140477245183808] Epoch[13] Batch [5]#011Speed: 533.74 samples/sec#011loss=0.979532\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:10 INFO 140477245183808] Epoch[13] Batch[10] avg_epoch_loss=1.017736\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:10 INFO 140477245183808] Epoch[13] Batch [10]#011Speed: 504.15 samples/sec#011loss=1.063581\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:10 INFO 140477245183808] processed a total of 328 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1118.3199882507324, \"sum\": 1118.3199882507324, \"min\": 1118.3199882507324}}, \"EndTime\": 1535047090.445742, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047089.327379}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:10 INFO 140477245183808] #throughput_metric: host=algo-1, train throughput=293.272457199 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:10 INFO 140477245183808] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:10 INFO 140477245183808] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:10 INFO 140477245183808] Saved checkpoint to \"/opt/ml/model/state_1b16dcd0-1f6f-4c6c-b1c2-1cb6bc614f06-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.01898193359375, \"sum\": 21.01898193359375, \"min\": 21.01898193359375}}, \"EndTime\": 1535047090.467066, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047090.445805}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:10 INFO 140477245183808] Epoch[14] Batch[0] avg_epoch_loss=0.734525\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:11 INFO 140477245183808] Epoch[14] Batch[5] avg_epoch_loss=1.024791\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:11 INFO 140477245183808] Epoch[14] Batch [5]#011Speed: 592.67 samples/sec#011loss=1.024791\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:11 INFO 140477245183808] Epoch[14] Batch[10] avg_epoch_loss=0.860850\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:11 INFO 140477245183808] Epoch[14] Batch [10]#011Speed: 560.84 samples/sec#011loss=0.664121\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:11 INFO 140477245183808] processed a total of 321 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1035.5441570281982, \"sum\": 1035.5441570281982, \"min\": 1035.5441570281982}}, \"EndTime\": 1535047091.502695, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047090.467114}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:11 INFO 140477245183808] #throughput_metric: host=algo-1, train throughput=309.959830071 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:11 INFO 140477245183808] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:11 INFO 140477245183808] best epoch loss so far\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/23/2018 17:58:11 INFO 140477245183808] Saved checkpoint to \"/opt/ml/model/state_f7b490d7-2581-497e-974e-d3d11437aec3-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.440101623535156, \"sum\": 20.440101623535156, \"min\": 20.440101623535156}}, \"EndTime\": 1535047091.523418, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047091.502745}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:12 INFO 140477245183808] Epoch[15] Batch[0] avg_epoch_loss=0.772145\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:12 INFO 140477245183808] Epoch[15] Batch[5] avg_epoch_loss=0.736896\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:12 INFO 140477245183808] Epoch[15] Batch [5]#011Speed: 592.28 samples/sec#011loss=0.736896\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:12 INFO 140477245183808] Epoch[15] Batch[10] avg_epoch_loss=0.826200\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:12 INFO 140477245183808] Epoch[15] Batch [10]#011Speed: 568.62 samples/sec#011loss=0.933365\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:12 INFO 140477245183808] processed a total of 348 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1052.1180629730225, \"sum\": 1052.1180629730225, \"min\": 1052.1180629730225}}, \"EndTime\": 1535047092.575625, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047091.523468}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:12 INFO 140477245183808] #throughput_metric: host=algo-1, train throughput=330.738424579 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:12 INFO 140477245183808] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:12 INFO 140477245183808] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:12 INFO 140477245183808] Saved checkpoint to \"/opt/ml/model/state_881ee2b7-c9b3-4902-a9d9-d3b4754e4f43-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.982027053833008, \"sum\": 20.982027053833008, \"min\": 20.982027053833008}}, \"EndTime\": 1535047092.596889, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047092.575675}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:13 INFO 140477245183808] Epoch[16] Batch[0] avg_epoch_loss=0.798252\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:13 INFO 140477245183808] Epoch[16] Batch[5] avg_epoch_loss=0.948928\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:13 INFO 140477245183808] Epoch[16] Batch [5]#011Speed: 585.38 samples/sec#011loss=0.948928\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:13 INFO 140477245183808] Epoch[16] Batch[10] avg_epoch_loss=0.879622\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:13 INFO 140477245183808] Epoch[16] Batch [10]#011Speed: 551.54 samples/sec#011loss=0.796455\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:13 INFO 140477245183808] processed a total of 330 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1048.6059188842773, \"sum\": 1048.6059188842773, \"min\": 1048.6059188842773}}, \"EndTime\": 1535047093.645581, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047092.596937}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:13 INFO 140477245183808] #throughput_metric: host=algo-1, train throughput=314.681365071 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:13 INFO 140477245183808] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:13 INFO 140477245183808] loss did not improve for 1 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:14 INFO 140477245183808] Epoch[17] Batch[0] avg_epoch_loss=0.813771\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:14 INFO 140477245183808] Epoch[17] Batch[5] avg_epoch_loss=0.920539\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:14 INFO 140477245183808] Epoch[17] Batch [5]#011Speed: 592.16 samples/sec#011loss=0.920539\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:14 INFO 140477245183808] processed a total of 308 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 974.5650291442871, \"sum\": 974.5650291442871, \"min\": 974.5650291442871}}, \"EndTime\": 1535047094.620402, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047093.645631}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:14 INFO 140477245183808] #throughput_metric: host=algo-1, train throughput=316.013147914 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:14 INFO 140477245183808] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:14 INFO 140477245183808] loss did not improve for 2 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:15 INFO 140477245183808] Epoch[18] Batch[0] avg_epoch_loss=1.025724\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:15 INFO 140477245183808] Epoch[18] Batch[5] avg_epoch_loss=0.847083\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:15 INFO 140477245183808] Epoch[18] Batch [5]#011Speed: 496.63 samples/sec#011loss=0.847083\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:15 INFO 140477245183808] processed a total of 312 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1063.2212162017822, \"sum\": 1063.2212162017822, \"min\": 1063.2212162017822}}, \"EndTime\": 1535047095.683904, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047094.620454}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:15 INFO 140477245183808] #throughput_metric: host=algo-1, train throughput=293.423392069 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:15 INFO 140477245183808] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:15 INFO 140477245183808] loss did not improve for 3 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:16 INFO 140477245183808] Epoch[19] Batch[0] avg_epoch_loss=1.014826\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:16 INFO 140477245183808] Epoch[19] Batch[5] avg_epoch_loss=0.867601\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:16 INFO 140477245183808] Epoch[19] Batch [5]#011Speed: 507.27 samples/sec#011loss=0.867601\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:16 INFO 140477245183808] Epoch[19] Batch[10] avg_epoch_loss=0.909847\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:16 INFO 140477245183808] Epoch[19] Batch [10]#011Speed: 493.06 samples/sec#011loss=0.960542\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:16 INFO 140477245183808] processed a total of 360 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1244.1020011901855, \"sum\": 1244.1020011901855, \"min\": 1244.1020011901855}}, \"EndTime\": 1535047096.928314, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047095.683966}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:16 INFO 140477245183808] #throughput_metric: host=algo-1, train throughput=289.346266331 records/second\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:16 INFO 140477245183808] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:16 INFO 140477245183808] loss did not improve for 4 epochs\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:16 INFO 140477245183808] Loading parameters from best epoch (15)\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.deserialize.time\": {\"count\": 1, \"max\": 8.295059204101562, \"sum\": 8.295059204101562, \"min\": 8.295059204101562}}, \"EndTime\": 1535047096.936925, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047096.928368}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:16 INFO 140477245183808] Final loss: 0.826199970462 (occurred at epoch 15)\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:16 INFO 140477245183808] #quality_metric: host=algo-1, train final_loss <loss>=0.826199970462\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:16 INFO 140477245183808] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:16 WARNING 140477245183808] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:16 INFO 140477245183808] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 1210.2279663085938, \"sum\": 1210.2279663085938, \"min\": 1210.2279663085938}}, \"EndTime\": 1535047098.147552, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047096.936977}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:19 INFO 140477245183808] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 2192.417860031128, \"sum\": 2192.417860031128, \"min\": 2192.417860031128}}, \"EndTime\": 1535047099.129716, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047098.147664}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:19 INFO 140477245183808] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:19 INFO 140477245183808] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 158.25891494750977, \"sum\": 158.25891494750977, \"min\": 158.25891494750977}}, \"EndTime\": 1535047099.288075, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047099.129765}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:19 INFO 140477245183808] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[31m[08/23/2018 17:58:19 INFO 140477245183808] No test data passed, skipping evaluation.\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 25891.350984573364, \"sum\": 25891.350984573364, \"min\": 25891.350984573364}, \"setuptime\": {\"count\": 1, \"max\": 7.740974426269531, \"sum\": 7.740974426269531, \"min\": 7.740974426269531}}, \"EndTime\": 1535047099.546476, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1535047099.288122}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2018-08-23 17:58:23 Uploading - Uploading generated training model\n",
      "2018-08-23 17:58:29 Completed - Training job completed\n",
      "Billable seconds: 136\n"
     ]
    }
   ],
   "source": [
    "for model_name, model_path in MODEL_PATHS.items():\n",
    "    \n",
    "    estimator = sagemaker.estimator.Estimator(\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        role='arn:aws:iam::223261615538:role/terraform-sagemaker-role',\n",
    "        image_name=DEEPAR_IMAGE,\n",
    "        train_instance_count=1,\n",
    "        train_instance_type='ml.m5.4xlarge',\n",
    "        base_job_name=model_name + '-marketplace-poc',\n",
    "        output_path=model_path,\n",
    "    )\n",
    "\n",
    "    hyperparameters  = {\n",
    "        \"time_freq\": freq,\n",
    "        \"context_length\": context_length,\n",
    "        \"prediction_length\": prediction_length,\n",
    "        \"num_cells\": \"32\",\n",
    "        \"num_layers\": \"2\",\n",
    "        \"likelihood\": \"student-t\",\n",
    "        \"epochs\": \"20\",\n",
    "        \"mini_batch_size\": \"32\",\n",
    "        \"learning_rate\": \"0.001\",\n",
    "        \"dropout_rate\": \"0.05\",\n",
    "        \"early_stopping_patience\": \"10\"\n",
    "    }\n",
    "\n",
    "    estimator.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "    data_channels = {\n",
    "        \"train\": \"{}/train/\".format(model_path),\n",
    "    }\n",
    "\n",
    "    estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an endpoint and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: http-latency-forecasting-marketplace-po-2018-08-23-17-50-12-874\n",
      "INFO:sagemaker:Creating endpoint-config with name http-latency-forecasting-marketplace-po-2018-08-23-17-50-12-874\n",
      "INFO:sagemaker:Creating endpoint with name http-latency-forecasting-marketplace-po-2018-08-23-17-50-12-874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "endpoint_name = sagemaker_session.endpoint_from_job(\n",
    "    job_name='http-latency-forecasting-marketplace-po-2018-08-23-17-50-12-874',\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    deployment_image=DEEPAR_IMAGE,\n",
    "    role='arn:aws:iam::223261615538:role/terraform-sagemaker-role'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "\n",
    "    def set_prediction_parameters(self, freq, prediction_length):\n",
    "        \"\"\"Set the time frequency and prediction length parameters. This method **must** be called\n",
    "        before being able to use `predict`.\n",
    "        \n",
    "        Parameters:\n",
    "        freq -- string indicating the time frequency\n",
    "        prediction_length -- integer, number of predicted time points\n",
    "        \n",
    "        Return value: none.\n",
    "        \"\"\"\n",
    "        self.freq = freq\n",
    "        self.prediction_length = prediction_length\n",
    "        \n",
    "    def predict(self, ts, encoding=\"utf-8\", num_samples=100, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        Parameters:\n",
    "        ts -- list of `pandas.Series` objects, the time series to predict\n",
    "        cat -- list of integers (default: None)\n",
    "        encoding -- string, encoding to use for the request (default: \"utf-8\")\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_times = [\"2018-08-22 13:36:10\"]\n",
    "        req = self.__encode_request(ts, encoding, num_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, prediction_times, encoding)\n",
    "    \n",
    "    def __encode_request(self, ts, encoding, num_samples, quantiles):\n",
    "        instances = [series_to_obj(ts)]\n",
    "        \n",
    "        configuration = {\"num_samples\": num_samples, \"output_types\": [\"quantiles\"], \"quantiles\": quantiles}\n",
    "        http_request_data = {\"instances\": instances, \"configuration\": configuration}\n",
    "        return json.dumps(http_request_data).encode(encoding)\n",
    "    \n",
    "    def __decode_response(self, response, prediction_times, encoding):\n",
    "        response_data = json.loads(response.decode(encoding))\n",
    "        list_of_df = []\n",
    "        for k in range(len(prediction_times)):\n",
    "            prediction_index = pd.DatetimeIndex(start=prediction_times[k], freq=self.freq, periods=self.prediction_length)\n",
    "            list_of_df.append(pd.DataFrame(data=response_data['predictions'][k]['quantiles'], index=prediction_index))\n",
    "        return list_of_df\n",
    "\n",
    "    \n",
    "def series_to_obj(ts):\n",
    "    obj = {\"start\": \"2018-08-22 13:36:10\", \"target\": ts}\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('./data/http_request_query_10.json', lines=True)\n",
    "df = df.set_index('start')['target']\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "timeseries_test, timeseries_training = [], []\n",
    "for newseries in df:\n",
    "    timeseries_test.append(newseries)\n",
    "    timeseries_training.append(newseries[:-prediction_length])\n",
    "    \n",
    "new_time_series_training = []\n",
    "for ts in timeseries_training:\n",
    "    new_time_series_training.append(ts)\n",
    "\n",
    "new_time_series_test = []\n",
    "for ts in timeseries_test:\n",
    "    new_time_series_test.append(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicto  = DeepARPredictor(\n",
    "    endpoint=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    content_type=\"application/json\"\n",
    ")\n",
    "\n",
    "predicto.set_prediction_parameters(freq, prediction_length)\n",
    "list_of_df  = predicto.predict(new_time_series_training[2])[0] # predicted forecast\n",
    "actual_data = pd.Series(df[2], index=pd.date_range('2018-08-22 13:36:10', periods=len(df[2]), freq='5s')) # full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: http-request-count-forecasting-marketpl-2018-08-23-17-54-34-508\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session.delete_endpoint(endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
